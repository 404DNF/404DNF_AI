{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8023,"status":"ok","timestamp":1758125895835,"user":{"displayName":"박현우","userId":"01265894329339476871"},"user_tz":-540},"id":"G6bxgXC6d_HH","outputId":"d1d01d48-92a8-4390-aea1-05e4e2ebcf87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1-SfUNu5d02W","outputId":"c676b3a3-ee76-436d-f5bf-21af92f75f58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_scatter-2.1.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_sparse-0.6.18%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_cluster-1.6.3%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_spline_conv-1.2.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.1)\n","Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n","Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n","Successfully installed torch-cluster-1.6.3+pt28cu126 torch-scatter-2.1.2+pt28cu126 torch-sparse-0.6.18+pt28cu126 torch-spline-conv-1.2.2+pt28cu126\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric\n","Successfully installed torch-geometric-2.6.1\n","Using device: cpu\n","✅ 데이터셋 로드 완료.\n","\n","====================\n","🚀 CONTEXTUAL 데이터셋 GNN 파이프라인 실행 시작\n","====================\n","결과 저장 경로: /content/drive/MyDrive/졸업논문/models/gnn_contextual_model\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-517508704.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['predicate'].fillna('Not Dark Pattern', inplace=True)\n"]},{"ename":"KeyError","evalue":"'sentence'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'sentence'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-517508704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mrun_gnn_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_save_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"⚠️ {name} 데이터셋이 비어있거나 로드되지 않아 건너뜁니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-517508704.py\u001b[0m in \u001b[0;36mrun_gnn_pipeline\u001b[0;34m(df, dataset_name, base_save_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# --- 2. 데이터 전처리 및 라벨 인코딩 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not Dark Pattern'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mlabels_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'sentence'"]}],"source":["# ======================================================\n","# Cell 1. 라이브러리 설치 및 기본 설정\n","# ======================================================\n","# PyTorch Geometric (PyG) 및 관련 라이브러리 설치\n","import torch\n","import os\n","\n","# PyTorch 버전에 맞는 PyG 라이브러리 설치\n","!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install torch-geometric\n","\n","# Hugging Face 및 기타 라이브러리 설치\n","!pip install -q transformers datasets scikit-learn pandas numpy matplotlib seaborn networkx\n","\n","# 기본 설정\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {DEVICE}\")\n","\n","# ======================================================\n","# Cell 2. 데이터 로드 및 전처리\n","# ======================================================\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","\n","# 데이터셋 경로 설정\n","base_path = '/content/drive/MyDrive/졸업논문/data/processed/'\n","contextual_path = os.path.join(base_path, 'contextual_merged.csv')\n","paraphrase_path = os.path.join(base_path, 'paraphrase_merged.csv')\n","template_path = os.path.join(base_path, 'template_merged.csv')\n","\n","# 데이터프레임 로드\n","try:\n","    contextual_df = pd.read_csv(contextual_path)\n","    paraphrase_df = pd.read_csv(paraphrase_path)\n","    template_df = pd.read_csv(template_path) # 변수명 오타 수정 (templete -> template)\n","    print(\"✅ 데이터셋 로드 완료.\")\n","except FileNotFoundError as e:\n","    print(f\"🚨 파일 로드 오류: {e}\")\n","    print(\"파일 경로를 다시 확인해주세요. 예: /content/drive/MyDrive/path/to/your/data.csv\")\n","    # 파일이 없으면 이후 코드 실행을 중단\n","    # 실제 환경에서는 이 부분에서 실행을 멈추는 것이 좋습니다.\n","    contextual_df, paraphrase_df, template_df = None, None, None\n","\n","datasets = {\n","    \"contextual\": contextual_df,\n","    \"paraphrase\": paraphrase_df,\n","    \"template\": template_df\n","}\n","\n","# ======================================================\n","# Cell 3. GNN 모델 및 학습 루프 정의\n","# ======================================================\n","from tqdm.notebook import tqdm\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.neighbors import NearestNeighbors\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import networkx as nx\n","from torch_geometric.utils import to_networkx\n","\n","# --- GCN 모델 정의 ---\n","class GCNClassifier(torch.nn.Module):\n","    def __init__(self, in_dim, hidden_dim, out_dim, n_layers=2, dropout=0.5):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(GCNConv(in_dim, hidden_dim))\n","        for _ in range(n_layers - 1):\n","            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n","        self.lin = torch.nn.Linear(hidden_dim, out_dim)\n","        self.dropout = dropout\n","\n","    def forward(self, x, edge_index):\n","        for conv in self.convs:\n","            x = conv(x, edge_index)\n","            x = F.relu(x)\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","        out = self.lin(x)\n","        return out\n","\n","# --- 평균 풀링 함수 ---\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0]\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","# --- 학습 및 평가 파이프라인 함수 ---\n","def run_gnn_pipeline(df, dataset_name, base_save_dir):\n","    print(\"\\n\" + \"=\"*20)\n","    print(f\"🚀 {dataset_name.upper()} 데이터셋 GNN 파이프라인 실행 시작\")\n","    print(\"=\"*20)\n","\n","    # --- 0. 파라미터 설정 ---\n","    EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n","    K_NEIGHBORS = 8\n","    HIDDEN_DIM = 128\n","    GNN_LAYERS = 2\n","    LR = 0.01\n","    EPOCHS = 200 # 학습 성능을 위해 Epochs 증가\n","    BATCH_SIZE = 32\n","\n","    # --- 1. 결과 저장 디렉토리 생성 ---\n","    SAVE_DIR = os.path.join(base_save_dir, f\"gnn_{dataset_name}_model\")\n","    os.makedirs(SAVE_DIR, exist_ok=True)\n","    print(f\"결과 저장 경로: {SAVE_DIR}\")\n","\n","    # --- 2. 데이터 전처리 및 라벨 인코딩 ---\n","    df['predicate'].fillna('Not Dark Pattern', inplace=True)\n","    sentences = df['sentence'].tolist()\n","    labels_raw = df['predicate'].tolist()\n","\n","    target_name = \"Not Dark Pattern\"\n","    binary_labels = [1 if label == target_name else 0 for label in labels_raw]\n","    labels_enc = np.array(binary_labels)\n","    num_classes = len(np.unique(labels_enc))\n","\n","    # LabelEncoder는 클래스 이름 시각화를 위해 사용\n","    le = LabelEncoder()\n","    le.fit([0, 1]) # 0: Dark Pattern, 1: Not Dark Pattern\n","    le.classes_ = np.array([f'Dark Pattern (0)', f'{target_name} (1)'])\n","\n","\n","    print(f\"데이터 샘플 수: {len(sentences)}\")\n","    print(f\"클래스 수: {num_classes}\")\n","    print(f\"클래스: {le.classes_}\")\n","\n","    # --- 3. 문장 임베딩 생성 ---\n","    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)\n","    embed_model = AutoModel.from_pretrained(EMBED_MODEL).to(DEVICE)\n","    embed_model.eval()\n","\n","    all_embeddings = []\n","    for i in tqdm(range(0, len(sentences), BATCH_SIZE), desc=f\"[{dataset_name}] Embedding\"):\n","        batch_texts = sentences[i:i+BATCH_SIZE]\n","        inputs = embed_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n","        with torch.no_grad():\n","            outputs = embed_model(**inputs)\n","            emb = mean_pooling(outputs, inputs['attention_mask'])\n","            emb = emb.cpu().numpy()\n","        all_embeddings.append(emb)\n","    all_embeddings = np.vstack(all_embeddings)\n","    print(f\"임베딩 Shape: {all_embeddings.shape}\")\n","\n","    # --- 4. kNN 그래프 생성 ---\n","    emb = all_embeddings\n","    n_neighbors = min(K_NEIGHBORS, len(emb) - 1)\n","    nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, metric=\"cosine\").fit(emb)\n","    _, indices = nbrs.kneighbors(emb)\n","\n","    edge_index_list = []\n","    for i in range(len(indices)):\n","        for j in indices[i][1:]:\n","            edge_index_list.append((i, j))\n","            edge_index_list.append((j, i))\n","    edge_index = np.array(edge_index_list).T\n","    print(f\"Edge index Shape: {edge_index.shape}\")\n","\n","    # --- 5. PyG Data 객체 생성 ---\n","    x = torch.tensor(emb, dtype=torch.float)\n","    y = torch.tensor(labels_enc, dtype=torch.long)\n","    edge_index_t = torch.tensor(edge_index, dtype=torch.long)\n","\n","    data = Data(x=x, edge_index=edge_index_t, y=y)\n","\n","    num_nodes = data.num_nodes\n","    perm = np.random.permutation(num_nodes)\n","    train_ratio = 0.8\n","    n_train = int(train_ratio * num_nodes)\n","    train_idx, val_idx = perm[:n_train], perm[n_train:]\n","\n","    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n","    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n","    data.train_mask[train_idx] = True\n","    data.val_mask[val_idx] = True\n","    print(data)\n","\n","    # --- 6. GNN 모델 초기화 ---\n","    model_gnn = GCNClassifier(in_dim=x.size(1), hidden_dim=HIDDEN_DIM, out_dim=num_classes, n_layers=GNN_LAYERS).to(DEVICE)\n","    optimizer = torch.optim.Adam(model_gnn.parameters(), lr=LR, weight_decay=1e-5)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    data = data.to(DEVICE)\n","\n","    # --- 7. 학습 루프 ---\n","    best_val_f1 = 0\n","    best_epoch = 0\n","    print(\"\\n GNN 모델 학습 시작...\")\n","    for epoch in range(1, EPOCHS + 1):\n","        model_gnn.train()\n","        optimizer.zero_grad()\n","        out = model_gnn(data.x, data.edge_index)\n","        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model_gnn.eval()\n","        with torch.no_grad():\n","            out_val = model_gnn(data.x, data.edge_index)\n","            preds_val = out_val[data.val_mask].argmax(dim=1).cpu().numpy()\n","            true_val = data.y[data.val_mask].cpu().numpy()\n","            val_acc = accuracy_score(true_val, preds_val)\n","            p, r, f, _ = precision_recall_fscore_support(true_val, preds_val, average='binary', zero_division=0)\n","\n","        if f > best_val_f1:\n","            best_val_f1 = f\n","            best_epoch = epoch\n","            torch.save(model_gnn.state_dict(), os.path.join(SAVE_DIR, \"gnn_model.pt\"))\n","\n","\n","        if epoch % 20 == 0:\n","            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {f:.4f}\")\n","\n","    print(f\"✅ 학습 완료. Best F1: {best_val_f1:.4f} at Epoch {best_epoch}\")\n","    print(f\"최적 모델 저장 완료: {os.path.join(SAVE_DIR, 'gnn_model.pt')}\")\n","\n","\n","    # --- 8. 최종 평가 및 결과 저장 ---\n","    # 최적 모델 로드\n","    model_gnn.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"gnn_model.pt\")))\n","    model_gnn.eval()\n","    with torch.no_grad():\n","        out_final = model_gnn(data.x, data.edge_index)\n","        preds = out_final.argmax(dim=1).cpu().numpy()\n","\n","    # 예측 결과 저장\n","    df_res = pd.DataFrame({\n","        \"sentence\": sentences,\n","        \"true_label\": labels_raw,\n","        \"true_label_enc\": labels_enc,\n","        \"pred_label_enc\": preds\n","    })\n","    pred_path = os.path.join(SAVE_DIR, \"gnn_predictions.csv\")\n","    df_res.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n","    print(f\"예측 결과 저장 완료: {pred_path}\")\n","\n","    # 최종 성능 리포트\n","    y_true_all = df_res['true_label_enc']\n","    y_pred_all = df_res['pred_label_enc']\n","    class_names = le.classes_\n","    report = classification_report(y_true_all, y_pred_all, target_names=[str(name) for name in class_names])\n","    print(\"\\n\" + \"=\"*15, \"최종 성능 평가 리포트\", \"=\"*15)\n","    print(report)\n","    print(\"=\"*55)\n","\n","    # Confusion Matrix 시각화\n","    cm = confusion_matrix(y_true_all, y_pred_all)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'GNN Model Confusion Matrix ({dataset_name})')\n","    cm_path = os.path.join(SAVE_DIR, \"gnn_confusion_matrix.png\")\n","    plt.savefig(cm_path)\n","    plt.show()\n","    print(f\"Confusion Matrix 저장 완료: {cm_path}\")\n","\n","    # kNN 그래프 시각화\n","    G = to_networkx(data.cpu(), to_undirected=True)\n","    color_map = ['#ff7f0e' if label == 0 else '#1f77b4' for label in data.y.cpu().numpy()]\n","    plt.figure(figsize=(15, 15))\n","    pos = nx.spring_layout(G, iterations=50, seed=42)\n","    nx.draw(G, pos, node_color=color_map, with_labels=False, node_size=60, width=0.5, edge_color='grey')\n","    plt.title(f'kNN Graph of Sentence Embeddings ({dataset_name})', fontsize=20)\n","\n","    blue_patch = plt.scatter([],[], c='#1f77b4', label=f'Class 1 ({target_name})')\n","    orange_patch = plt.scatter([],[], c='#ff7f0e', label=f'Class 0 (Dark Pattern)')\n","    plt.legend(handles=[blue_patch, orange_patch], loc='upper right', fontsize=14, title='Labels')\n","\n","    graph_path = os.path.join(SAVE_DIR, \"gnn_graph.png\")\n","    plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n","    plt.show()\n","    print(f\"GNN 그래프 저장 완료: {graph_path}\")\n","    print(f\"🎉 {dataset_name.upper()} 데이터셋 파이프라인 종료.\")\n","\n","\n","# ======================================================\n","# Cell 4. 메인 실행 루프\n","# ======================================================\n","base_save_directory = '/content/drive/MyDrive/졸업논문/models/'\n","\n","if all(df is not None for df in datasets.values()):\n","    for name, df in datasets.items():\n","        if df is not None and not df.empty:\n","            run_gnn_pipeline(df.copy(), name, base_save_directory)\n","        else:\n","            print(f\"⚠️ {name} 데이터셋이 비어있거나 로드되지 않아 건너뜁니다.\")\n","else:\n","    print(\"🚨 데이터셋 로드에 실패하여 GNN 파이프라인을 실행할 수 없습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lf6ZMnBcd3ms"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1nmXhZ0o0ECPmv5vxyRBcPFccR8aDQauF","authorship_tag":"ABX9TyNkus+fn3bpGevFdYkAB20N"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}