{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2350e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch=2.3.1 | pyg=2.5.3 | device=mps\n",
      "DATA_PATH exists: True\n",
      "OUTDIR: ../resgcn_runs\n",
      "HP: {'hidden': 128, 'layers': 2, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 1e-05, 'knn_k': 10, 'epochs': 60}\n",
      "Embedder: sentence-transformers/all-mpnet-base-v2 | mutual_kNN=True | cache=embeddings_mpnet.npy\n",
      "Loss: label_smoothing=0.05, class_weight_clip=(0.5,2.0)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 1 — Setup & Config (Improved: MPNet + mutual kNN + smoothing)\n",
    "\n",
    "import os, sys, json, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score)\n",
    "\n",
    "# Torch / PyG\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm  # PyG 2.5.x\n",
    "\n",
    "# Embed/Plot\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# --------------------\n",
    "# Repro & Device\n",
    "# --------------------\n",
    "RNG_SEED = 42\n",
    "random.seed(RNG_SEED); np.random.seed(RNG_SEED); torch.manual_seed(RNG_SEED)\n",
    "\n",
    "FORCE_CPU = False\n",
    "if (not FORCE_CPU) and torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\"); torch.cuda.manual_seed_all(RNG_SEED)\n",
    "elif (not FORCE_CPU) and hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"torch={torch.__version__} | pyg={torch_geometric.__version__} | device={DEVICE}\")\n",
    "\n",
    "# --------------------\n",
    "# Paths (internal only)\n",
    "#  - 네가 사용하던 경로를 그대로 씀\n",
    "# --------------------\n",
    "DATA_PATH = Path('../data/0923preprocessed/template_predicate.csv')\n",
    "OUTDIR = Path('../resgcn_runs')\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------\n",
    "# Columns\n",
    "# --------------------\n",
    "TEXT_COL       = 'String'\n",
    "PREDICATE_COL  = 'predicate'\n",
    "TYPE_COL       = 'Type'   # optional\n",
    "\n",
    "# --------------------\n",
    "# Labels\n",
    "# --------------------\n",
    "NOT_DP_NAME = 'Not Dark Pattern'\n",
    "EXPECTED_CLASS_COUNT = 10\n",
    "ND_SYN = {\n",
    "    \"none\",\"null\",\"nan\",\"na\",\"n/a\",\"\",\"-\",\"_\",\n",
    "    \"no dp\",\"notdp\",\"not a dark pattern\",\"not dark pattern\",\n",
    "    \"non dark pattern\",\"nondarkpattern\",\n",
    "    \"비다크\",\"비 다크\",\"없음\",\"해당없음\",\"해당 없음\",\"미해당\",\"무\"\n",
    "}\n",
    "\n",
    "# --------------------\n",
    "# Embedding (UPGRADED)\n",
    "# --------------------\n",
    "EMBEDDER_NAME = 'sentence-transformers/all-mpnet-base-v2'   # ★ 업그레이드 포인트\n",
    "EMB_TAG = 'mpnet'                                           # 캐시 구분 태그\n",
    "KNN_METRIC = 'cosine'\n",
    "DEFAULT_KNN_K = 10\n",
    "USE_MUTUAL_KNN = True                                       # ★ mutual kNN 사용\n",
    "EMBED_CACHE = OUTDIR / f'embeddings_{EMB_TAG}.npy'\n",
    "LABMAP_JSON = OUTDIR / 'label_mapping.json'\n",
    "\n",
    "# --------------------\n",
    "# Split ratios\n",
    "# --------------------\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.7, 0.1, 0.2\n",
    "\n",
    "# --------------------\n",
    "# Improved baseline HP (이전 튜닝 베스트를 기본값으로)\n",
    "# --------------------\n",
    "IMPROVED_HP = dict(\n",
    "    hidden=128,\n",
    "    layers=2,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    knn_k=DEFAULT_KNN_K,\n",
    "    epochs=60\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Loss tweaks\n",
    "# --------------------\n",
    "LABEL_SMOOTHING = 0.05    # ★ label smoothing\n",
    "W_CLIP_MIN, W_CLIP_MAX = 0.5, 2.0  # 1/sqrt(freq) 가중치 클리핑 범위\n",
    "\n",
    "print(\"DATA_PATH exists:\", DATA_PATH.exists())\n",
    "print(\"OUTDIR:\", OUTDIR)\n",
    "print(\"HP:\", IMPROVED_HP)\n",
    "print(f\"Embedder: {EMBEDDER_NAME} | mutual_kNN={USE_MUTUAL_KNN} | cache={EMBED_CACHE.name}\")\n",
    "print(f\"Loss: label_smoothing={LABEL_SMOOTHING}, class_weight_clip=({W_CLIP_MIN},{W_CLIP_MAX})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e66f470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['String', 'Type', 'label', 'predicate']\n",
      "Rows: 3200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>String</th>\n",
       "      <th>Type</th>\n",
       "      <th>label</th>\n",
       "      <th>predicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seen on product page 79 clicks today</td>\n",
       "      <td>Social Proof</td>\n",
       "      <td>1</td>\n",
       "      <td>Activity Notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flash Sale ends in SHOP NOW</td>\n",
       "      <td>Urgency</td>\n",
       "      <td>1</td>\n",
       "      <td>Limited-time Messages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 people have viewed this wine today</td>\n",
       "      <td>Social Proof</td>\n",
       "      <td>1</td>\n",
       "      <td>Activity Notifications</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  String          Type  label  \\\n",
       "0   Seen on product page 79 clicks today  Social Proof      1   \n",
       "1            Flash Sale ends in SHOP NOW       Urgency      1   \n",
       "2  17 people have viewed this wine today  Social Proof      1   \n",
       "\n",
       "                predicate  \n",
       "0  Activity Notifications  \n",
       "1   Limited-time Messages  \n",
       "2  Activity Notifications  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicate counts (after normalize):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predicate\n",
       "Not Dark Pattern                    1600\n",
       "Activity Notifications               361\n",
       "Low-stock Messages                   360\n",
       "Limited-time Messages                240\n",
       "Countdown Timers                     160\n",
       "Pressured Selling                    157\n",
       "Confirmshaming                       137\n",
       "Trick Questions                      106\n",
       "High-demand Messages                  40\n",
       "Testimonials of Uncertain Origin      39\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 클래스 10개 확인.\n",
      "num_classes: 10\n",
      "classes: ['Activity Notifications', 'Confirmshaming', 'Countdown Timers', 'High-demand Messages', 'Limited-time Messages', 'Low-stock Messages', 'Not Dark Pattern', 'Pressured Selling', 'Testimonials of Uncertain Origin', 'Trick Questions']\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 2 — Load data, normalize labels → enforce 10 classes (Improved run)\n",
    "\n",
    "import re, json\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Load\n",
    "assert DATA_PATH.exists(), f\"데이터 없음: {DATA_PATH}\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# 2) 유연 컬럼 매핑 (대소문자/공백/변형 허용)\n",
    "lower = {c.lower(): c for c in df.columns}\n",
    "assert 'string' in lower and 'predicate' in lower, \\\n",
    "    f\"CSV에 'String', 'predicate' 열이 필요합니다. 현재 열: {df.columns.tolist()}\"\n",
    "\n",
    "df = df.rename(columns={\n",
    "    lower['string']: TEXT_COL,\n",
    "    lower['predicate']: PREDICATE_COL\n",
    "})\n",
    "if 'type' in lower:\n",
    "    df = df.rename(columns={lower['type']: TYPE_COL})\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Rows:\", len(df))\n",
    "display(df.head(3))\n",
    "\n",
    "# 3) 라벨 정규화: None/동의어 -> Not Dark Pattern\n",
    "def normalize_pred(v):\n",
    "    if pd.isna(v):\n",
    "        return NOT_DP_NAME\n",
    "    s_raw = str(v).strip()\n",
    "    s = s_raw.lower()\n",
    "    s_simple = re.sub(r\"[‐-‒–—\\-_/\\\\\\.\\(\\)\\[\\]\\s]+\", \"\", s)  # 기호/공백 제거 버전\n",
    "    if (s in ND_SYN) or (s_simple in ND_SYN):\n",
    "        return NOT_DP_NAME\n",
    "    return s_raw\n",
    "\n",
    "df[PREDICATE_COL] = df[PREDICATE_COL].map(normalize_pred)\n",
    "\n",
    "# 4) 클래스 분포 확인 + 10개 강제 체크\n",
    "vc = df[PREDICATE_COL].value_counts()\n",
    "print(\"\\nPredicate counts (after normalize):\")\n",
    "display(vc)\n",
    "\n",
    "assert NOT_DP_NAME in vc.index, f\"'{NOT_DP_NAME}' 라벨이 없습니다. ND_SYN 세트를 보강하세요.\"\n",
    "\n",
    "uniq = vc.index.nunique()\n",
    "if uniq != EXPECTED_CLASS_COUNT:\n",
    "    print(f\"⚠️ 클래스 개수 기대={EXPECTED_CLASS_COUNT}, 현재={uniq}\")\n",
    "    print(\"현재 라벨 목록:\", sorted(vc.index.tolist()))\n",
    "    raise AssertionError(\"Class count mismatch. CSV 표기/동의어를 점검하세요.\")\n",
    "else:\n",
    "    print(f\"✅ 클래스 {EXPECTED_CLASS_COUNT}개 확인.\")\n",
    "\n",
    "# 5) LabelEncoder 적합 + 매핑 저장\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[PREDICATE_COL].astype(str))\n",
    "\n",
    "with open(LABMAP_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump({i: c for i, c in enumerate(le.classes_)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"classes:\", list(le.classes_))\n",
    "\n",
    "# 이후 셀에서 df, y, le, num_classes 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "828423c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13/13 [00:12<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings and cached: (3200, 768) -> ../resgcn_runs/embeddings_mpnet.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 3 — Text → Embeddings with MPNet (cache)\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if EMBED_CACHE.exists():\n",
    "    X = np.load(EMBED_CACHE)\n",
    "    print(f\"Loaded cached embeddings: {X.shape} from {EMBED_CACHE}\")\n",
    "else:\n",
    "    st_model = SentenceTransformer(EMBEDDER_NAME, device=str(DEVICE))\n",
    "    texts = df[TEXT_COL].astype(str).tolist()\n",
    "    X = st_model.encode(\n",
    "        texts,\n",
    "        batch_size=256,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,  # cosine kNN에 유리\n",
    "    )\n",
    "    np.save(EMBED_CACHE, X)\n",
    "    print(f\"Computed embeddings and cached: {X.shape} -> {EMBED_CACHE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f61dca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes=3200, directed_edges=17208, undirected_pairs≈8604, avg_degree≈5.38\n",
      "data.x: (3200, 768), data.y: (3200,), edge_index: (2, 17208)\n",
      "mutual_kNN=ON | k=10\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 4 — Build kNN graph (mutual-kNN) and make PyG Data\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- kNN helpers ---\n",
    "def build_knn_indices(embeddings: np.ndarray, k: int, metric: str):\n",
    "    \"\"\"Return neighbor index array of shape (N, k) excluding self.\"\"\"\n",
    "    assert k >= 1, \"k must be >= 1\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric)\n",
    "    nbrs.fit(embeddings)\n",
    "    _, idxs = nbrs.kneighbors(embeddings)\n",
    "    return idxs[:, 1:]  # drop self\n",
    "\n",
    "def build_edge_index_from_neighbors(neigh_idx: np.ndarray, mutual: bool = False):\n",
    "    \"\"\"\n",
    "    neigh_idx: (N, k) int array\n",
    "    mutual=False: standard kNN -> make directed edges i->j, and also add reverse to treat as undirected.\n",
    "    mutual=True : keep edge (i,j) only if i in N(j) and j in N(i); add both directions for PyG.\n",
    "    \"\"\"\n",
    "    N, K = neigh_idx.shape\n",
    "    if not mutual:\n",
    "        rows = np.repeat(np.arange(N), K)\n",
    "        cols = neigh_idx.reshape(-1)\n",
    "        # make undirected by adding reverse\n",
    "        ei = np.vstack([np.concatenate([rows, cols]), np.concatenate([cols, rows])])\n",
    "        # unique columns\n",
    "        ei = np.unique(ei, axis=1)\n",
    "        return ei\n",
    "    else:\n",
    "        # Build neighbor sets for mutual test\n",
    "        neigh_sets = [set(neigh_idx[i].tolist()) for i in range(N)]\n",
    "        undirected_pairs = []\n",
    "        for i in range(N):\n",
    "            for j in neigh_sets[i]:\n",
    "                if i < j and i in neigh_sets[j]:\n",
    "                    undirected_pairs.append((i, j))\n",
    "        undirected_pairs = np.array(undirected_pairs, dtype=np.int64)\n",
    "        if undirected_pairs.size == 0:\n",
    "            # Fallback: if mutual is too strict and yields empty graph, revert to standard kNN\n",
    "            rows = np.repeat(np.arange(N), K)\n",
    "            cols = neigh_idx.reshape(-1)\n",
    "            ei = np.vstack([np.concatenate([rows, cols]), np.concatenate([cols, rows])])\n",
    "            ei = np.unique(ei, axis=1)\n",
    "            return ei\n",
    "\n",
    "        # expand to directed for PyG: (i,j) and (j,i)\n",
    "        rows = np.concatenate([undirected_pairs[:,0], undirected_pairs[:,1]])\n",
    "        cols = np.concatenate([undirected_pairs[:,1], undirected_pairs[:,0]])\n",
    "        ei = np.vstack([rows, cols])\n",
    "        ei = np.unique(ei, axis=1)\n",
    "        return ei\n",
    "\n",
    "def make_pyg_data(X_np: np.ndarray, y_np: np.ndarray, edge_index_np: np.ndarray, device: torch.device):\n",
    "    data = Data(\n",
    "        x=torch.tensor(X_np, dtype=torch.float32),\n",
    "        y=torch.tensor(y_np, dtype=torch.long),\n",
    "        edge_index=torch.tensor(edge_index_np, dtype=torch.long),\n",
    "    )\n",
    "    return data.to(device)\n",
    "\n",
    "# --- build graph with mutual-kNN (if enabled) ---\n",
    "k_use = IMPROVED_HP['knn_k']\n",
    "idxs = build_knn_indices(X, k=k_use, metric=KNN_METRIC)\n",
    "edge_index_np = build_edge_index_from_neighbors(idxs, mutual=USE_MUTUAL_KNN)\n",
    "\n",
    "data = make_pyg_data(X, y, edge_index_np, DEVICE)\n",
    "\n",
    "# --- stats print ---\n",
    "N = X.shape[0]\n",
    "directed_E = edge_index_np.shape[1]\n",
    "# estimate undirected pair count (since we added both directions)\n",
    "undirected_pairs = directed_E // 2\n",
    "avg_degree = directed_E / N  # since directed includes both directions, this equals undirected*2/N\n",
    "print(f\"nodes={N}, directed_edges={directed_E}, undirected_pairs≈{undirected_pairs}, avg_degree≈{avg_degree:.2f}\")\n",
    "print(f\"data.x: {tuple(data.x.shape)}, data.y: {tuple(data.y.shape)}, edge_index: {tuple(data.edge_index.shape)}\")\n",
    "print(f\"mutual_kNN={'ON' if USE_MUTUAL_KNN else 'OFF'} | k={k_use}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "603a6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total=3200 | train=2239 | val=320 | test=641\n",
      "Disjoint?  True\n",
      "Cover all? True\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 5 — Train/Val/Test masks (7:1:2, stratified)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = len(y)\n",
    "indices = np.arange(N)\n",
    "\n",
    "# 1) train vs (val+test)\n",
    "train_idx, tmp_idx, y_train, y_tmp = train_test_split(\n",
    "    indices, y, test_size=(1 - TRAIN_RATIO),\n",
    "    random_state=RNG_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# 2) val vs test (0.1 : 0.2 유지)\n",
    "val_rel = VAL_RATIO / (VAL_RATIO + TEST_RATIO)  # 0.1 / 0.3 = 0.333...\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(\n",
    "    tmp_idx, y_tmp, test_size=(1 - val_rel),\n",
    "    random_state=RNG_SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "# boolean masks\n",
    "train_mask = torch.zeros(N, dtype=torch.bool); train_mask[train_idx] = True\n",
    "val_mask   = torch.zeros(N, dtype=torch.bool); val_mask[val_idx]   = True\n",
    "test_mask  = torch.zeros(N, dtype=torch.bool); test_mask[test_idx] = True\n",
    "\n",
    "# attach to PyG data (on DEVICE)\n",
    "data.train_mask = train_mask.to(DEVICE)\n",
    "data.val_mask   = val_mask.to(DEVICE)\n",
    "data.test_mask  = test_mask.to(DEVICE)\n",
    "\n",
    "print(f\"Total={N} | train={train_mask.sum().item()} | val={val_mask.sum().item()} | test={test_mask.sum().item()}\")\n",
    "print(\"Disjoint? \", (train_mask & val_mask).sum().item()==0 and\n",
    "                   (train_mask & test_mask).sum().item()==0 and\n",
    "                   (val_mask & test_mask).sum().item()==0)\n",
    "print(\"Cover all?\", (train_mask | val_mask | test_mask).sum().item()==N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d540858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts per class: [253.0, 96.0, 112.0, 28.0, 168.0, 252.0, 1119.0, 110.0, 27.0, 74.0]\n",
      "Moderated class weights: [0.615, 0.998, 0.924, 1.848, 0.755, 0.616, 0.5, 0.932, 1.882, 1.137]\n",
      "Label smoothing ε=0.05\n",
      "ResGCN(\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResidualGCNBlock(\n",
      "      (conv): GCNConv(768, 128)\n",
      "      (bn): BatchNorm(128)\n",
      "      (res_proj): Linear(in_features=768, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): ResidualGCNBlock(\n",
      "      (conv): GCNConv(128, 128)\n",
      "      (bn): BatchNorm(128)\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Total parameters: 215,178\n",
      "Ready for improved training.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 6 — ResGCN model + moderated class weights & label smoothing (setup)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm  # PyG 2.5.x\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------------\n",
    "# 1) ResGCN 정의\n",
    "# ------------------------\n",
    "class ResidualGCNBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(dim_in, dim_out, improved=True)\n",
    "        self.bn = BatchNorm(dim_out)\n",
    "        self.dropout = dropout\n",
    "        self.res_proj = nn.Linear(dim_in, dim_out) if dim_in != dim_out else None\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        identity = x\n",
    "        out = self.conv(x, edge_index)\n",
    "        out = self.bn(out)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        if self.res_proj is not None:\n",
    "            identity = self.res_proj(identity)\n",
    "        return out + identity\n",
    "\n",
    "class ResGCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim, layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        dims = [in_dim] + [hidden] * layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualGCNBlock(dims[i], dims[i+1], dropout=dropout) for i in range(layers)\n",
    "        ])\n",
    "        self.head = nn.Linear(hidden, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, ei)\n",
    "        return self.head(x)\n",
    "\n",
    "# ------------------------\n",
    "# 2) 클래스 가중치 (1/sqrt(freq), 평균=1 정규화, [0.5, 2.0] 클리핑)\n",
    "# ------------------------\n",
    "train_labels = data.y[data.train_mask].detach().cpu().numpy()\n",
    "counts = np.bincount(train_labels, minlength=num_classes).astype(float)\n",
    "inv_sqrt = 1.0 / np.sqrt(np.maximum(counts, 1.0))\n",
    "class_weights_np = inv_sqrt / inv_sqrt.mean()\n",
    "class_weights_np = np.clip(class_weights_np, W_CLIP_MIN, W_CLIP_MAX)\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "print(\"Train counts per class:\", counts.tolist())\n",
    "print(\"Moderated class weights:\", np.round(class_weights_np, 3).tolist())\n",
    "print(f\"Label smoothing ε={LABEL_SMOOTHING}\")\n",
    "\n",
    "# ------------------------\n",
    "# 3) 모델/옵티마/손실 세팅\n",
    "# ------------------------\n",
    "model = ResGCN(\n",
    "    in_dim=data.x.size(1),\n",
    "    hidden=int(IMPROVED_HP['hidden']),\n",
    "    out_dim=num_classes,\n",
    "    layers=int(IMPROVED_HP['layers']),\n",
    "    dropout=float(IMPROVED_HP['dropout'])\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=float(IMPROVED_HP['lr']),\n",
    "    weight_decay=float(IMPROVED_HP['weight_decay'])\n",
    ")\n",
    "\n",
    "# torch>=1.10: F.cross_entropy에 label_smoothing 있음 → 클래스 가중치와 함께 사용\n",
    "def ce_with_smoothing(logits, targets):\n",
    "    return F.cross_entropy(\n",
    "        logits, targets,\n",
    "        weight=class_weights,\n",
    "        label_smoothing=float(LABEL_SMOOTHING)\n",
    "    )\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"Ready for improved training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "754c4f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Imp] epoch 001  loss=2.4344  trainF1=0.1406  valF1=0.1404\n",
      "[Imp] epoch 002  loss=2.0355  trainF1=0.3328  valF1=0.3497\n",
      "[Imp] epoch 003  loss=1.7192  trainF1=0.4150  valF1=0.4126\n",
      "[Imp] epoch 004  loss=1.4785  trainF1=0.4397  valF1=0.4296\n",
      "[Imp] epoch 005  loss=1.3057  trainF1=0.4507  valF1=0.4465\n",
      "[Imp] epoch 006  loss=1.1704  trainF1=0.4518  valF1=0.4546\n",
      "[Imp] epoch 007  loss=1.0740  trainF1=0.4564  valF1=0.4590\n",
      "[Imp] epoch 008  loss=1.0010  trainF1=0.4543  valF1=0.4622\n",
      "[Imp] epoch 009  loss=0.9466  trainF1=0.4558  valF1=0.4632\n",
      "[Imp] epoch 010  loss=0.8976  trainF1=0.4566  valF1=0.4638\n",
      "[Imp] epoch 011  loss=0.8651  trainF1=0.4549  valF1=0.4541\n",
      "[Imp] epoch 012  loss=0.8360  trainF1=0.4545  valF1=0.4447\n",
      "[Imp] epoch 013  loss=0.8133  trainF1=0.4613  valF1=0.4665\n",
      "[Imp] epoch 014  loss=0.7903  trainF1=0.4668  valF1=0.4695\n",
      "[Imp] epoch 015  loss=0.7644  trainF1=0.4856  valF1=0.4695\n",
      "[Imp] epoch 016  loss=0.7516  trainF1=0.4905  valF1=0.4704\n",
      "[Imp] epoch 017  loss=0.7354  trainF1=0.5346  valF1=0.4746\n",
      "[Imp] epoch 018  loss=0.7215  trainF1=0.5393  valF1=0.4767\n",
      "[Imp] epoch 019  loss=0.7139  trainF1=0.5536  valF1=0.4700\n",
      "[Imp] epoch 020  loss=0.7040  trainF1=0.5961  valF1=0.5091\n",
      "[Imp] epoch 021  loss=0.6875  trainF1=0.6090  valF1=0.5183\n",
      "[Imp] epoch 022  loss=0.6788  trainF1=0.6457  valF1=0.5202\n",
      "[Imp] epoch 023  loss=0.6710  trainF1=0.6771  valF1=0.5494\n",
      "[Imp] epoch 024  loss=0.6630  trainF1=0.7001  valF1=0.5679\n",
      "[Imp] epoch 025  loss=0.6540  trainF1=0.7155  valF1=0.5760\n",
      "[Imp] epoch 026  loss=0.6481  trainF1=0.7543  valF1=0.6004\n",
      "[Imp] epoch 027  loss=0.6417  trainF1=0.7690  valF1=0.6219\n",
      "[Imp] epoch 028  loss=0.6376  trainF1=0.7860  valF1=0.6352\n",
      "[Imp] epoch 029  loss=0.6312  trainF1=0.8107  valF1=0.6494\n",
      "[Imp] epoch 030  loss=0.6232  trainF1=0.8268  valF1=0.6624\n",
      "[Imp] epoch 031  loss=0.6212  trainF1=0.8413  valF1=0.6959\n",
      "[Imp] epoch 032  loss=0.6130  trainF1=0.8575  valF1=0.7141\n",
      "[Imp] epoch 033  loss=0.6083  trainF1=0.8688  valF1=0.7544\n",
      "[Imp] epoch 034  loss=0.6042  trainF1=0.8741  valF1=0.7569\n",
      "[Imp] epoch 035  loss=0.5983  trainF1=0.8814  valF1=0.7660\n",
      "[Imp] epoch 036  loss=0.5936  trainF1=0.8875  valF1=0.7645\n",
      "[Imp] epoch 037  loss=0.5933  trainF1=0.8963  valF1=0.7691\n",
      "[Imp] epoch 038  loss=0.5848  trainF1=0.9006  valF1=0.7708\n",
      "[Imp] epoch 039  loss=0.5807  trainF1=0.9028  valF1=0.7708\n",
      "[Imp] epoch 040  loss=0.5753  trainF1=0.9065  valF1=0.7496\n",
      "[Imp] epoch 041  loss=0.5737  trainF1=0.9121  valF1=0.7553\n",
      "[Imp] epoch 042  loss=0.5700  trainF1=0.9163  valF1=0.7573\n",
      "[Imp] epoch 043  loss=0.5649  trainF1=0.9183  valF1=0.7650\n",
      "[Imp] epoch 044  loss=0.5621  trainF1=0.9229  valF1=0.7704\n",
      "[Imp] epoch 045  loss=0.5617  trainF1=0.9271  valF1=0.7762\n",
      "[Imp] epoch 046  loss=0.5573  trainF1=0.9303  valF1=0.7762\n",
      "[Imp] epoch 047  loss=0.5539  trainF1=0.9327  valF1=0.7779\n",
      "[Imp] epoch 048  loss=0.5529  trainF1=0.9347  valF1=0.7856\n",
      "[Imp] epoch 049  loss=0.5477  trainF1=0.9381  valF1=0.7856\n",
      "[Imp] epoch 050  loss=0.5456  trainF1=0.9398  valF1=0.7827\n",
      "[Imp] epoch 051  loss=0.5448  trainF1=0.9388  valF1=0.7865\n",
      "[Imp] epoch 052  loss=0.5434  trainF1=0.9371  valF1=0.7881\n",
      "[Imp] epoch 053  loss=0.5379  trainF1=0.9406  valF1=0.7856\n",
      "[Imp] epoch 054  loss=0.5357  trainF1=0.9414  valF1=0.7856\n",
      "[Imp] epoch 055  loss=0.5332  trainF1=0.9409  valF1=0.7899\n",
      "[Imp] epoch 056  loss=0.5315  trainF1=0.9409  valF1=0.7862\n",
      "[Imp] epoch 057  loss=0.5298  trainF1=0.9430  valF1=0.7839\n",
      "[Imp] epoch 058  loss=0.5315  trainF1=0.9460  valF1=0.7839\n",
      "[Imp] epoch 059  loss=0.5247  trainF1=0.9483  valF1=0.7839\n",
      "[Imp] epoch 060  loss=0.5238  trainF1=0.9490  valF1=0.7839\n",
      "\n",
      "[Improved] Finished in 4.9s\n",
      "# Internal Test (macro) — Improved\n",
      "     ACC: 0.9392\n",
      "    PREC: 0.8822\n",
      "     REC: 0.8753\n",
      "      F1: 0.8768\n",
      " ROC_AUC: 0.9907\n",
      "  PR_AUC: 0.9221\n",
      "Saved CM -> ../resgcn_runs/cm_internal_improved.png\n",
      "Saved classification report -> ../resgcn_runs/classification_report_Improved.txt\n",
      "Saved improved model -> ../resgcn_runs/resgcn_improved.pt (0.83 MB)\n",
      "Saved metrics -> ../resgcn_runs/metrics_internal_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 7 — Train (improved) + internal test metrics/CM + save model & reports\n",
    "\n",
    "import time, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, classification_report)\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- helpers --------\n",
    "def evaluate_subset(logits_subset, y_true_subset, n_classes: int):\n",
    "    y_true = y_true_subset.detach().cpu().numpy()\n",
    "    y_pred = logits_subset.argmax(-1).detach().cpu().numpy()\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # macro AUCs (OvR)\n",
    "    try:\n",
    "        y_true_ovr = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        y_score = F.softmax(logits_subset, dim=-1).detach().cpu().numpy()\n",
    "        roc = roc_auc_score(y_true_ovr, y_score, average='macro', multi_class='ovr')\n",
    "        pr  = average_precision_score(y_true_ovr, y_score, average='macro')\n",
    "    except Exception:\n",
    "        roc, pr = float('nan'), float('nan')\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc_auc=roc, pr_auc=pr, cm=cm)\n",
    "\n",
    "# -------- train --------\n",
    "PATIENCE = 8  # early stopping patience\n",
    "best_val, best_state = -1.0, None\n",
    "stale = 0\n",
    "hist = []\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(1, int(IMPROVED_HP['epochs']) + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(data)\n",
    "    loss = ce_with_smoothing(logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_eval = model(data)\n",
    "        tr = evaluate_subset(logits_eval[data.train_mask], data.y[data.train_mask], num_classes)\n",
    "        va = evaluate_subset(logits_eval[data.val_mask],   data.y[data.val_mask],   num_classes)\n",
    "\n",
    "    hist.append((ep, tr['f1'], va['f1']))\n",
    "    print(f\"[Imp] epoch {ep:03d}  loss={loss.item():.4f}  trainF1={tr['f1']:.4f}  valF1={va['f1']:.4f}\")\n",
    "\n",
    "    if va['f1'] > best_val + 1e-6:\n",
    "        best_val = va['f1']\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale >= PATIENCE:\n",
    "            print(\"Early stopping (improved).\")\n",
    "            break\n",
    "\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "# -------- eval on test --------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_final = model(data)\n",
    "test_metrics = evaluate_subset(logits_final[data.test_mask], data.y[data.test_mask], num_classes)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n[Improved] Finished in {elapsed:.1f}s\")\n",
    "print(\"# Internal Test (macro) — Improved\")\n",
    "for k in ['acc','prec','rec','f1','roc_auc','pr_auc']:\n",
    "    print(f\"{k.upper():>8}: {test_metrics[k]:.4f}\")\n",
    "\n",
    "# -------- save confusion matrix --------\n",
    "cm = test_metrics['cm']\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(cm, interpolation='nearest'); ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_title('Confusion Matrix — Internal Test (Improved)')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "ax.set_xticks(range(num_classes)); ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([c[:14] for c in list(le.classes_)], rotation=45, ha='right')\n",
    "ax.set_yticklabels([c[:14] for c in list(le.classes_)])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, int(v), ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "cm_path = OUTDIR / 'cm_internal_improved.png'\n",
    "plt.savefig(cm_path, dpi=220); plt.close(fig)\n",
    "print(\"Saved CM ->\", cm_path)\n",
    "\n",
    "# -------- save classification report --------\n",
    "y_true_test = data.y[data.test_mask].detach().cpu().numpy()\n",
    "y_pred_test = logits_final[data.test_mask].argmax(-1).detach().cpu().numpy()\n",
    "cr_txt = classification_report(y_true_test, y_pred_test, target_names=list(le.classes_), zero_division=0)\n",
    "cr_path = OUTDIR / 'classification_report_Improved.txt'\n",
    "with open(cr_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(cr_txt)\n",
    "print(\"Saved classification report ->\", cr_path)\n",
    "\n",
    "# -------- save model (with configs) --------\n",
    "ckpt_path = OUTDIR / 'resgcn_improved.pt'\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'hp': IMPROVED_HP,\n",
    "    'knn_k': int(IMPROVED_HP['knn_k']),\n",
    "    'config': {\n",
    "        'in_dim': int(data.x.size(1)),\n",
    "        'out_dim': int(num_classes),\n",
    "        'label_mapping': dict(enumerate([str(c) for c in list(le.classes_)])),\n",
    "        'metric': KNN_METRIC,\n",
    "        'embedder': EMBEDDER_NAME,\n",
    "        'mutual_kNN': True\n",
    "    },\n",
    "    'class_weights': class_weights.detach().cpu().numpy().tolist(),\n",
    "    'label_smoothing': float(LABEL_SMOOTHING)\n",
    "}, ckpt_path)\n",
    "size_mb = ckpt_path.stat().st_size / (1024*1024)\n",
    "print(f\"Saved improved model -> {ckpt_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# -------- append/overwrite summary CSV/JSON --------\n",
    "summary_row = {\n",
    "    \"Model\": \"Improved\",\n",
    "    \"acc\": test_metrics['acc'],\n",
    "    \"prec\": test_metrics['prec'],\n",
    "    \"rec\": test_metrics['rec'],\n",
    "    \"f1\": test_metrics['f1'],\n",
    "    \"roc_auc\": test_metrics['roc_auc'],\n",
    "    \"pr_auc\": test_metrics['pr_auc'],\n",
    "    \"ModelSizeMB\": round(size_mb, 2),\n",
    "    \"kNN_k\": int(IMPROVED_HP['knn_k']),\n",
    "    \"embedder\": EMBEDDER_NAME,\n",
    "    \"mutual_kNN\": True,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING\n",
    "}\n",
    "# CSV/JSON 저장\n",
    "import pandas as pd\n",
    "csv_path = OUTDIR / 'metrics_internal_summary.csv'\n",
    "if csv_path.exists():\n",
    "    df_old = pd.read_csv(csv_path)\n",
    "    df_new = pd.concat([df_old, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "else:\n",
    "    df_new = pd.DataFrame([summary_row])\n",
    "df_new.to_csv(csv_path, index=False)\n",
    "with open(OUTDIR / 'metrics_internal_summary.json', 'w') as f:\n",
    "    json.dump(df_new.to_dict(orient='records'), f, indent=2)\n",
    "print(\"Saved metrics ->\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79564e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exact duplicate check] test 내 train과 완전 동일 문장 비율: 4.524%  (count=29/641)\n",
      "[Near-dup] cos≥0.90: 19.969%  (128/641)\n",
      "[Near-dup] cos≥0.95: 11.700%  (75/641)\n",
      "[Near-dup] cos≥0.98: 6.552%  (42/641)\n",
      "[Near-dup] cos≥0.99: 6.084%  (39/641)\n",
      "Saved potential near-duplicates -> ../resgcn_runs/leak_audit_top_matches.csv\n",
      "Saved class-wise near-dup summary -> ../resgcn_runs/leak_audit_by_class.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "      <th>mean_max_sim</th>\n",
       "      <th>p95_max_sim</th>\n",
       "      <th>p99_max_sim</th>\n",
       "      <th>frac_ge_0.98</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Low-stock Messages</td>\n",
       "      <td>72</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Countdown Timers</td>\n",
       "      <td>32</td>\n",
       "      <td>0.846843</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confirmshaming</td>\n",
       "      <td>27</td>\n",
       "      <td>0.802925</td>\n",
       "      <td>0.997182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High-demand Messages</td>\n",
       "      <td>8</td>\n",
       "      <td>0.807432</td>\n",
       "      <td>0.986048</td>\n",
       "      <td>0.990256</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited-time Messages</td>\n",
       "      <td>48</td>\n",
       "      <td>0.810750</td>\n",
       "      <td>0.988279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trick Questions</td>\n",
       "      <td>21</td>\n",
       "      <td>0.717859</td>\n",
       "      <td>0.955315</td>\n",
       "      <td>0.983358</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Activity Notifications</td>\n",
       "      <td>72</td>\n",
       "      <td>0.717467</td>\n",
       "      <td>0.959712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not Dark Pattern</td>\n",
       "      <td>321</td>\n",
       "      <td>0.638126</td>\n",
       "      <td>0.906987</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.015576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pressured Selling</td>\n",
       "      <td>32</td>\n",
       "      <td>0.672717</td>\n",
       "      <td>0.867191</td>\n",
       "      <td>0.894432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Testimonials of Uncertain Origin</td>\n",
       "      <td>8</td>\n",
       "      <td>0.637128</td>\n",
       "      <td>0.873781</td>\n",
       "      <td>0.924070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              class  count  mean_max_sim  p95_max_sim  \\\n",
       "5                Low-stock Messages     72      0.909665     1.000001   \n",
       "2                  Countdown Timers     32      0.846843     0.997878   \n",
       "1                    Confirmshaming     27      0.802925     0.997182   \n",
       "3              High-demand Messages      8      0.807432     0.986048   \n",
       "4             Limited-time Messages     48      0.810750     0.988279   \n",
       "9                   Trick Questions     21      0.717859     0.955315   \n",
       "0            Activity Notifications     72      0.717467     0.959712   \n",
       "6                  Not Dark Pattern    321      0.638126     0.906987   \n",
       "7                 Pressured Selling     32      0.672717     0.867191   \n",
       "8  Testimonials of Uncertain Origin      8      0.637128     0.873781   \n",
       "\n",
       "   p99_max_sim  frac_ge_0.98  \n",
       "5     1.000001      0.250000  \n",
       "2     1.000000      0.218750  \n",
       "1     1.000000      0.185185  \n",
       "3     0.990256      0.125000  \n",
       "4     1.000000      0.062500  \n",
       "9     0.983358      0.047619  \n",
       "0     1.000000      0.027778  \n",
       "6     0.999790      0.015576  \n",
       "7     0.894432      0.000000  \n",
       "8     0.924070      0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 8 — Sanity check: 중복/유사문장(누수) 점검 (train ↔ test)\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) 인덱스 준비\n",
    "train_idx = np.where(data.train_mask.detach().cpu().numpy())[0]\n",
    "val_idx   = np.where(data.val_mask.detach().cpu().numpy())[0]\n",
    "test_idx  = np.where(data.test_mask.detach().cpu().numpy())[0]\n",
    "\n",
    "# 2) 문자열 정규화(소문자, 공백/기호 최소화)로 \"완전 중복\" 탐지\n",
    "def norm_txt(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    # 너무 과격하게 지우면 다른 문장도 뭉개질 수 있어 최소한만\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "tr_norm = pd.Series(df.loc[train_idx, TEXT_COL].map(norm_txt).values, index=train_idx)\n",
    "te_norm = pd.Series(df.loc[test_idx,  TEXT_COL].map(norm_txt).values, index=test_idx)\n",
    "\n",
    "tr_set = set(tr_norm.values.tolist())\n",
    "exact_dups = te_norm[te_norm.isin(tr_set)]\n",
    "exact_dup_rate = len(exact_dups) / len(te_norm) if len(te_norm) else 0.0\n",
    "print(f\"[Exact duplicate check] test 내 train과 완전 동일 문장 비율: {exact_dup_rate:.3%}  (count={len(exact_dups)}/{len(te_norm)})\")\n",
    "\n",
    "# 3) 임베딩 기반 유사도(코사인)로 \"근접 중복\" 탐지\n",
    "#    - 각 test 문장에 대해 train 임베딩과의 cosine sim 최대값을 계산\n",
    "#    - MPNet 임베딩 X는 이미 L2-normalize 되어 있으므로 dot이 곧 cosine\n",
    "X_tr = X[train_idx]\n",
    "X_te = X[test_idx]\n",
    "sims = X_te @ X_tr.T                              # (n_test, n_train)\n",
    "max_sim = sims.max(axis=1)\n",
    "argmax_tr = sims.argmax(axis=1)\n",
    "\n",
    "# 임계치별 요약\n",
    "for thr in [0.90, 0.95, 0.98, 0.99]:\n",
    "    rate = float((max_sim >= thr).mean())\n",
    "    print(f\"[Near-dup] cos≥{thr:.2f}: {rate:.3%}  ({int((max_sim>=thr).sum())}/{len(max_sim)})\")\n",
    "\n",
    "# 4) 상위 의심 케이스 테이블 저장\n",
    "top_k = min(100, len(test_idx))\n",
    "order = np.argsort(-max_sim)[:top_k]\n",
    "sus = pd.DataFrame({\n",
    "    \"test_idx\": test_idx[order],\n",
    "    \"train_match_idx\": train_idx[argmax_tr[order]],\n",
    "    \"cos_sim\": max_sim[order],\n",
    "    \"test_text\": df.loc[test_idx[order], TEXT_COL].values,\n",
    "    \"train_text\": df.loc[train_idx[argmax_tr[order]], TEXT_COL].values,\n",
    "    \"test_label\": df.loc[test_idx[order], PREDICATE_COL].values,\n",
    "    \"train_label\": df.loc[train_idx[argmax_tr[order]], PREDICATE_COL].values,\n",
    "})\n",
    "out_csv = OUTDIR / \"leak_audit_top_matches.csv\"\n",
    "sus.to_csv(out_csv, index=False)\n",
    "print(\"Saved potential near-duplicates ->\", out_csv)\n",
    "\n",
    "# 5) 클래스별 유사도 분포 요약(테스트 기준)\n",
    "test_labels = y[test_idx]\n",
    "df_sim = pd.DataFrame({\"label\": test_labels, \"max_sim\": max_sim})\n",
    "class_names = list(le.classes_)\n",
    "cls_rows = []\n",
    "for ci, cname in enumerate(class_names):\n",
    "    arr = df_sim.loc[df_sim[\"label\"]==ci, \"max_sim\"].values\n",
    "    if len(arr)==0:\n",
    "        continue\n",
    "    cls_rows.append({\n",
    "        \"class\": cname,\n",
    "        \"count\": len(arr),\n",
    "        \"mean_max_sim\": float(np.mean(arr)),\n",
    "        \"p95_max_sim\": float(np.percentile(arr, 95)),\n",
    "        \"p99_max_sim\": float(np.percentile(arr, 99)),\n",
    "        \"frac_ge_0.98\": float((arr>=0.98).mean()),\n",
    "    })\n",
    "cls_df = pd.DataFrame(cls_rows).sort_values(\"frac_ge_0.98\", ascending=False)\n",
    "cls_csv = OUTDIR / \"leak_audit_by_class.csv\"\n",
    "cls_df.to_csv(cls_csv, index=False)\n",
    "print(\"Saved class-wise near-dup summary ->\", cls_csv)\n",
    "\n",
    "display(cls_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8856251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test size: 599/641  (removed 42 with cos≥0.98)\n",
      "                           class  test_count  removed  kept  rm_rate\n",
      "              Low-stock Messages          72       18    54 0.250000\n",
      "                Countdown Timers          32        7    25 0.218750\n",
      "                  Confirmshaming          27        5    22 0.185185\n",
      "            High-demand Messages           8        1     7 0.125000\n",
      "           Limited-time Messages          48        3    45 0.062500\n",
      "                 Trick Questions          21        1    20 0.047619\n",
      "          Activity Notifications          72        2    70 0.027778\n",
      "                Not Dark Pattern         321        5   316 0.015576\n",
      "               Pressured Selling          32        0    32 0.000000\n",
      "Testimonials of Uncertain Origin           8        0     8 0.000000\n",
      "\n",
      "# Internal Test (CLEAN, cos<thr)\n",
      "     ACC: 0.9366\n",
      "    PREC: 0.8732\n",
      "     REC: 0.8659\n",
      "      F1: 0.8674\n",
      " ROC_AUC: 0.9899\n",
      "  PR_AUC: 0.9150\n",
      "Saved CM -> ../resgcn_runs/cm_internal_improved_clean0p98.png\n",
      "Saved classification report -> ../resgcn_runs/classification_report_Improved_clean0p98.txt\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 9a — Clean Test evaluation (exclude near-dup ≥ 0.98 vs train)\n",
    "\n",
    "import numpy as np, pandas as pd, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score, average_precision_score,\n",
    "                             classification_report)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 0) 보조: 평가 함수\n",
    "def eval_pack_logits(logits, y_true, n_classes: int):\n",
    "    y_np  = y_true.detach().cpu().numpy()\n",
    "    yhat  = logits.argmax(-1).detach().cpu().numpy()\n",
    "    acc   = accuracy_score(y_np, yhat)\n",
    "    prec  = precision_score(y_np, yhat, average='macro', zero_division=0)\n",
    "    rec   = recall_score(y_np, yhat, average='macro', zero_division=0)\n",
    "    f1    = f1_score(y_np, yhat, average='macro', zero_division=0)\n",
    "    try:\n",
    "        y_ovr  = label_binarize(y_np, classes=np.arange(n_classes))\n",
    "        y_prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        roc    = roc_auc_score(y_ovr, y_prob, average='macro', multi_class='ovr')\n",
    "        pr     = average_precision_score(y_ovr, y_prob, average='macro')\n",
    "    except Exception:\n",
    "        roc, pr = float('nan'), float('nan')\n",
    "    cm    = confusion_matrix(y_np, yhat)\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc_auc=roc, pr_auc=pr, cm=cm,\n",
    "                y_true=y_np, y_pred=yhat)\n",
    "\n",
    "# 1) Improved 모델로 전체 그래프에서 logits 계산\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_full = model(data)\n",
    "\n",
    "# 2) 필요 시 max_sim 재계산 (Cell 8을 건너뛴 경우 대비)\n",
    "try:\n",
    "    max_sim\n",
    "    train_idx\n",
    "    test_idx\n",
    "except NameError:\n",
    "    train_idx = np.where(data.train_mask.detach().cpu().numpy())[0]\n",
    "    test_idx  = np.where(data.test_mask.detach().cpu().numpy())[0]\n",
    "    X_tr, X_te = X[train_idx], X[test_idx]\n",
    "    sims = X_te @ X_tr.T\n",
    "    max_sim = sims.max(axis=1)\n",
    "\n",
    "# 3) clean mask (near-dup ≥ 0.98 제거)\n",
    "thr = 0.98\n",
    "clean_keep = max_sim < thr\n",
    "clean_ids  = np.where(data.test_mask.detach().cpu().numpy())[0][clean_keep]\n",
    "drop_ids   = np.where(data.test_mask.detach().cpu().numpy())[0][~clean_keep]\n",
    "\n",
    "print(f\"Clean test size: {clean_keep.sum()}/{len(clean_keep)}  (removed {len(drop_ids)} with cos≥{thr})\")\n",
    "\n",
    "# 클래스별 제거 비율 요약\n",
    "y_test_all = y[data.test_mask.detach().cpu().numpy()]\n",
    "removed = pd.DataFrame({\n",
    "    \"label\": y_test_all[~clean_keep]\n",
    "})\n",
    "keeped  = pd.DataFrame({\n",
    "    \"label\": y_test_all[clean_keep]\n",
    "})\n",
    "by_cls = []\n",
    "for ci, cname in enumerate(list(le.classes_)):\n",
    "    tot = int((y_test_all==ci).sum())\n",
    "    rm  = int((removed[\"label\"]==ci).sum())\n",
    "    kp  = int((keeped[\"label\"]==ci).sum())\n",
    "    by_cls.append({\"class\": cname, \"test_count\": tot, \"removed\": rm, \"kept\": kp, \"rm_rate\": (rm/tot if tot else 0.0)})\n",
    "cls_tbl = pd.DataFrame(by_cls).sort_values(\"rm_rate\", ascending=False)\n",
    "print(cls_tbl.to_string(index=False))\n",
    "\n",
    "# 4) clean test 로지츠/라벨 추출 → 지표/CM/리포트\n",
    "mask_clean = torch.zeros(len(y), dtype=torch.bool, device=logits_full.device)\n",
    "mask_clean[clean_ids] = True\n",
    "metrics_clean = eval_pack_logits(logits_full[mask_clean], data.y[mask_clean], num_classes)\n",
    "\n",
    "print(\"\\n# Internal Test (CLEAN, cos<thr)\")\n",
    "for k in ['acc','prec','rec','f1','roc_auc','pr_auc']:\n",
    "    print(f\"{k.upper():>8}: {metrics_clean[k]:.4f}\")\n",
    "\n",
    "# 혼동행렬 저장\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(metrics_clean['cm'], interpolation='nearest'); ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_title(f'Confusion Matrix — Clean Test (cos<{thr})')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "ax.set_xticks(range(num_classes)); ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([c[:14] for c in list(le.classes_)], rotation=45, ha='right')\n",
    "ax.set_yticklabels([c[:14] for c in list(le.classes_)])\n",
    "for (i,j), v in np.ndenumerate(metrics_clean['cm']):\n",
    "    ax.text(j, i, int(v), ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "cm_clean_path = OUTDIR / f'cm_internal_improved_clean{str(thr).replace(\".\",\"p\")}.png'\n",
    "plt.savefig(cm_clean_path, dpi=220); plt.close(fig)\n",
    "print(\"Saved CM ->\", cm_clean_path)\n",
    "\n",
    "# 분류 리포트 저장\n",
    "cr_txt = classification_report(metrics_clean['y_true'], metrics_clean['y_pred'],\n",
    "                               target_names=list(le.classes_), zero_division=0)\n",
    "cr_path = OUTDIR / f'classification_report_Improved_clean{str(thr).replace(\".\",\"p\")}.txt'\n",
    "with open(cr_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(cr_txt)\n",
    "print(\"Saved classification report ->\", cr_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1163632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../resgcn_runs/graph_internal_full_improved.png\n",
      "Saved: ../resgcn_runs/graph_internal_test_overlay_improved.png\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 10 — Internal graph visuals (Improved: full + test overlay)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "def draw_full(coords: np.ndarray, edge_index_np: np.ndarray, labels: np.ndarray,\n",
    "              class_names, out_path: Path, title: str):\n",
    "    fig = plt.figure(figsize=(16, 10)); ax = fig.add_subplot(111); ax.set_title(title)\n",
    "\n",
    "    # 엣지(연하게)\n",
    "    for (u, v) in zip(edge_index_np[0], edge_index_np[1]):\n",
    "        ax.plot([coords[u,0], coords[v,0]], [coords[u,1], coords[v,1]], alpha=0.02)\n",
    "\n",
    "    # 노드(클래스별)\n",
    "    for ci, cname in enumerate(class_names):\n",
    "        idx = np.where(labels == ci)[0]\n",
    "        if len(idx):\n",
    "            ax.scatter(coords[idx,0], coords[idx,1], s=8, alpha=0.85, label=cname)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), frameon=False)\n",
    "    ax.axis('off'); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=220, bbox_inches='tight'); plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "def draw_test_overlay(coords: np.ndarray, edge_index_np: np.ndarray, labels: np.ndarray,\n",
    "                      test_mask_tensor, class_names, out_path: Path, title: str):\n",
    "    fig = plt.figure(figsize=(16, 10)); ax = fig.add_subplot(111); ax.set_title(title)\n",
    "\n",
    "    # 엣지(연하게)\n",
    "    for (u, v) in zip(edge_index_np[0], edge_index_np[1]):\n",
    "        ax.plot([coords[u,0], coords[v,0]], [coords[u,1], coords[v,1]], alpha=0.02)\n",
    "\n",
    "    # 배경 전체(회색, 희미하게)\n",
    "    ax.scatter(coords[:,0], coords[:,1], s=6, alpha=0.12, c='grey')\n",
    "\n",
    "    # 테스트 노드만 크게 + 검은테두리\n",
    "    tm = test_mask_tensor.detach().cpu().numpy().astype(bool)\n",
    "    for ci, cname in enumerate(class_names):\n",
    "        idx = np.where((labels == ci) & tm)[0]\n",
    "        if len(idx):\n",
    "            ax.scatter(coords[idx,0], coords[idx,1],\n",
    "                       s=140, edgecolor='k', linewidths=0.9, label=cname)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), frameon=False)\n",
    "    ax.axis('off'); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=240, bbox_inches='tight'); plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "# Improved 러닝의 edge_index_np / data / y / le가 이미 세션에 존재.\n",
    "# 좌표가 아직 없으면 생성\n",
    "if 'coords_improved' not in globals():\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(X.shape[0]))\n",
    "    G.add_edges_from(list(zip(edge_index_np[0].tolist(), edge_index_np[1].tolist())))\n",
    "    pos = nx.spring_layout(G, seed=RNG_SEED, iterations=50)\n",
    "    coords_improved = np.array([pos[i] for i in range(X.shape[0])])\n",
    "\n",
    "full_path    = OUTDIR / 'graph_internal_full_improved.png'\n",
    "overlay_path = OUTDIR / 'graph_internal_test_overlay_improved.png'\n",
    "\n",
    "draw_full(coords_improved, edge_index_np, y, list(le.classes_),\n",
    "          full_path, f\"kNN Graph (mutual={USE_MUTUAL_KNN}, k={IMPROVED_HP['knn_k']}) — all nodes\")\n",
    "draw_test_overlay(coords_improved, edge_index_np, y, data.test_mask,\n",
    "                  list(le.classes_), overlay_path, \"Internal Test Overlay — Improved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "507000aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning 14 trials...\n",
      "[1/14] valF1=0.2727 valAcc=0.675  hp={'hidden': 256, 'layers': 4, 'dropout': 0.5, 'lr': 0.001, 'weight_decay': 0.001, 'knn_k': 10}\n",
      "[2/14] valF1=0.4166 valAcc=0.750  hp={'hidden': 256, 'layers': 2, 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 1e-05, 'knn_k': 8}\n",
      "[3/14] valF1=0.5203 valAcc=0.822  hp={'hidden': 256, 'layers': 2, 'dropout': 0.5, 'lr': 0.0005, 'weight_decay': 1e-05, 'knn_k': 12}\n",
      "[4/14] valF1=0.8258 valAcc=0.925  hp={'hidden': 128, 'layers': 3, 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 0.001, 'knn_k': 8}\n",
      "[5/14] valF1=0.7779 valAcc=0.912  hp={'hidden': 128, 'layers': 3, 'dropout': 0.5, 'lr': 0.0005, 'weight_decay': 0.0005, 'knn_k': 15}\n",
      "[6/14] valF1=0.4264 valAcc=0.784  hp={'hidden': 256, 'layers': 4, 'dropout': 0.5, 'lr': 0.0005, 'weight_decay': 1e-05, 'knn_k': 10}\n",
      "[7/14] valF1=0.7752 valAcc=0.912  hp={'hidden': 128, 'layers': 2, 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 1e-05, 'knn_k': 12}\n",
      "[8/14] valF1=0.4020 valAcc=0.750  hp={'hidden': 256, 'layers': 4, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 1e-05, 'knn_k': 10}\n",
      "[9/14] valF1=0.8229 valAcc=0.934  hp={'hidden': 128, 'layers': 3, 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 0.001, 'knn_k': 10}\n",
      "[10/14] valF1=0.3655 valAcc=0.744  hp={'hidden': 128, 'layers': 4, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 0.001, 'knn_k': 10}\n",
      "[11/14] valF1=0.5025 valAcc=0.794  hp={'hidden': 256, 'layers': 2, 'dropout': 0.5, 'lr': 0.001, 'weight_decay': 1e-05, 'knn_k': 15}\n",
      "[12/14] valF1=0.8268 valAcc=0.922  hp={'hidden': 128, 'layers': 2, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 0.001, 'knn_k': 15}\n",
      "[13/14] valF1=0.3466 valAcc=0.722  hp={'hidden': 256, 'layers': 4, 'dropout': 0.1, 'lr': 0.0005, 'weight_decay': 0.0005, 'knn_k': 12}\n",
      "[14/14] valF1=0.4270 valAcc=0.681  hp={'hidden': 128, 'layers': 4, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 0.0005, 'knn_k': 10}\n",
      "\n",
      "Saved tuning summary -> ../resgcn_runs/tuning_trials_improved.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>hidden</th>\n",
       "      <th>layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>knn_k</th>\n",
       "      <th>sec</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.826849</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.841466</td>\n",
       "      <td>0.921997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.825769</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.838005</td>\n",
       "      <td>0.917317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>10</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.822920</td>\n",
       "      <td>0.934375</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>0.928237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.777945</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.837959</td>\n",
       "      <td>0.918877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>12</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.775151</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.830548</td>\n",
       "      <td>0.917317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.520326</td>\n",
       "      <td>0.821875</td>\n",
       "      <td>0.461690</td>\n",
       "      <td>0.787832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>15</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.502480</td>\n",
       "      <td>0.793750</td>\n",
       "      <td>0.466161</td>\n",
       "      <td>0.765991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.427038</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.357869</td>\n",
       "      <td>0.639626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.426357</td>\n",
       "      <td>0.784375</td>\n",
       "      <td>0.377386</td>\n",
       "      <td>0.751950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.416649</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.396484</td>\n",
       "      <td>0.730109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial  hidden  layers  dropout      lr  weight_decay  knn_k  sec  \\\n",
       "11     12     128       2      0.1  0.0010       0.00100     15  3.3   \n",
       "3       4     128       3      0.1  0.0005       0.00100      8  3.8   \n",
       "8       9     128       3      0.1  0.0005       0.00100     10  3.7   \n",
       "4       5     128       3      0.5  0.0005       0.00050     15  4.3   \n",
       "6       7     128       2      0.1  0.0005       0.00001     12  3.3   \n",
       "2       3     256       2      0.5  0.0005       0.00001     12  1.5   \n",
       "10     11     256       2      0.5  0.0010       0.00001     15  1.4   \n",
       "13     14     128       4      0.1  0.0010       0.00050     10  0.9   \n",
       "5       6     256       4      0.5  0.0005       0.00001     10  1.5   \n",
       "1       2     256       2      0.1  0.0005       0.00001      8  1.2   \n",
       "\n",
       "      val_f1   val_acc   test_f1  test_acc  \n",
       "11  0.826849  0.921875  0.841466  0.921997  \n",
       "3   0.825769  0.925000  0.838005  0.917317  \n",
       "8   0.822920  0.934375  0.844765  0.928237  \n",
       "4   0.777945  0.912500  0.837959  0.918877  \n",
       "6   0.775151  0.912500  0.830548  0.917317  \n",
       "2   0.520326  0.821875  0.461690  0.787832  \n",
       "10  0.502480  0.793750  0.466161  0.765991  \n",
       "13  0.427038  0.681250  0.357869  0.639626  \n",
       "5   0.426357  0.784375  0.377386  0.751950  \n",
       "1   0.416649  0.750000  0.396484  0.730109  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Best HP: {'hidden': 128, 'layers': 2, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 0.001, 'knn_k': 15}\n",
      "\n",
      "# Internal Test (macro) — Improved+BestHP\n",
      "     ACC: 0.6490\n",
      "    PREC: 0.3103\n",
      "     REC: 0.2448\n",
      "      F1: 0.2420\n",
      " ROC_AUC: 0.9607\n",
      "  PR_AUC: 0.7564\n",
      "Saved CM -> ../resgcn_runs/cm_internal_improved_besthp.png\n",
      "Saved model -> ../resgcn_runs/resgcn_improved_besthp.pt (0.83 MB)\n",
      "Updated metrics -> ../resgcn_runs/metrics_internal_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>ModelSizeMB</th>\n",
       "      <th>kNN_k</th>\n",
       "      <th>embedder</th>\n",
       "      <th>mutual_kNN</th>\n",
       "      <th>label_smoothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Improved</td>\n",
       "      <td>0.939158</td>\n",
       "      <td>0.882171</td>\n",
       "      <td>0.875349</td>\n",
       "      <td>0.876785</td>\n",
       "      <td>0.990689</td>\n",
       "      <td>0.922132</td>\n",
       "      <td>0.83</td>\n",
       "      <td>10</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Improved+BestHP</td>\n",
       "      <td>0.648986</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.244827</td>\n",
       "      <td>0.241957</td>\n",
       "      <td>0.960709</td>\n",
       "      <td>0.756355</td>\n",
       "      <td>0.83</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model       acc      prec       rec        f1   roc_auc  \\\n",
       "0         Improved  0.939158  0.882171  0.875349  0.876785  0.990689   \n",
       "1  Improved+BestHP  0.648986  0.310345  0.244827  0.241957  0.960709   \n",
       "\n",
       "     pr_auc  ModelSizeMB  kNN_k                                 embedder  \\\n",
       "0  0.922132         0.83     10  sentence-transformers/all-mpnet-base-v2   \n",
       "1  0.756355         0.83     15  sentence-transformers/all-mpnet-base-v2   \n",
       "\n",
       "   mutual_kNN  label_smoothing  \n",
       "0        True             0.05  \n",
       "1        True             0.05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 11 — Hyperparameter Tuning (Improved pipeline) + retrain best\n",
    "\n",
    "import time, itertools, random\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score, average_precision_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 검색 공간 ----------\n",
    "SPACE = {\n",
    "    \"hidden\":       [128, 256],\n",
    "    \"layers\":       [2, 3, 4],\n",
    "    \"dropout\":      [0.1, 0.3, 0.5],\n",
    "    \"lr\":           [1e-3, 5e-4],\n",
    "    \"weight_decay\": [1e-5, 5e-4, 1e-3],\n",
    "    \"knn_k\":        [8, 10, 12, 15],\n",
    "}\n",
    "N_TRIALS = 14   # 부담되면 줄여도 OK\n",
    "PATIENCE  = 8\n",
    "\n",
    "def evaluate_subset(logits, y_true, n_classes):\n",
    "    y_np = y_true.detach().cpu().numpy()\n",
    "    y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "    acc  = accuracy_score(y_np, y_pred)\n",
    "    prec = precision_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    rec  = recall_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    f1   = f1_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    try:\n",
    "        y_ovr = label_binarize(y_np, classes=np.arange(n_classes))\n",
    "        y_prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        roc = roc_auc_score(y_ovr, y_prob, average='macro', multi_class='ovr')\n",
    "        pr  = average_precision_score(y_ovr, y_prob, average='macro')\n",
    "    except Exception:\n",
    "        roc, pr = float('nan'), float('nan')\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc_auc=roc, pr_auc=pr)\n",
    "\n",
    "def run_one_trial(hp):\n",
    "    # 1) 그래프 재구성 (mutual kNN, k=hp['knn_k'])\n",
    "    idxs = build_knn_indices(X, k=int(hp['knn_k']), metric=KNN_METRIC)\n",
    "    ei_np = build_edge_index_from_neighbors(idxs, mutual=True)\n",
    "\n",
    "    data_k = Data(\n",
    "        x=data.x.detach().clone(),\n",
    "        y=data.y.detach().clone(),\n",
    "        edge_index=torch.tensor(ei_np, dtype=torch.long, device=DEVICE),\n",
    "    )\n",
    "    data_k.train_mask = data.train_mask\n",
    "    data_k.val_mask   = data.val_mask\n",
    "    data_k.test_mask  = data.test_mask\n",
    "\n",
    "    # 2) 모델/옵티마\n",
    "    model_k = ResGCN(\n",
    "        in_dim=data_k.x.size(1),\n",
    "        hidden=int(hp['hidden']),\n",
    "        out_dim=num_classes,\n",
    "        layers=int(hp['layers']),\n",
    "        dropout=float(hp['dropout'])\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        model_k.parameters(),\n",
    "        lr=float(hp['lr']),\n",
    "        weight_decay=float(hp['weight_decay'])\n",
    "    )\n",
    "\n",
    "    # 3) 학습\n",
    "    best_val, best_state, stale = -1.0, None, 0\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, int(IMPROVED_HP['epochs']) + 1):\n",
    "        model_k.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model_k(data_k)\n",
    "        loss = ce_with_smoothing(logits[data_k.train_mask], data_k.y[data_k.train_mask])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model_k.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_eval = model_k(data_k)\n",
    "            val = evaluate_subset(logits_eval[data_k.val_mask], data_k.y[data_k.val_mask], num_classes)\n",
    "        if val['f1'] > best_val + 1e-6:\n",
    "            best_val = val['f1']\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model_k.state_dict().items()}\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    # 4) 베스트 로드 후 test 평가\n",
    "    if best_state is not None:\n",
    "        model_k.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "    model_k.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_final = model_k(data_k)\n",
    "        val_metrics  = evaluate_subset(logits_final[data_k.val_mask],  data_k.y[data_k.val_mask],  num_classes)\n",
    "        test_metrics = evaluate_subset(logits_final[data_k.test_mask], data_k.y[data_k.test_mask], num_classes)\n",
    "\n",
    "    sec = time.time() - t0\n",
    "    return val_metrics, test_metrics, sec, ei_np, model_k\n",
    "\n",
    "# --------- 샘플링 목록 만들기 (랜덤 탐색) ----------\n",
    "grid = list(itertools.product(\n",
    "    SPACE['hidden'], SPACE['layers'], SPACE['dropout'],\n",
    "    SPACE['lr'], SPACE['weight_decay'], SPACE['knn_k']\n",
    "))\n",
    "random.Random(RNG_SEED).shuffle(grid)\n",
    "cands = grid[:N_TRIALS]\n",
    "\n",
    "records = []\n",
    "best_row = None\n",
    "best_model = None\n",
    "best_ei_np = None\n",
    "\n",
    "print(f\"Tuning {len(cands)} trials...\")\n",
    "for t, (hidden, layers, dropout, lr, weight_decay, knn_k) in enumerate(cands, 1):\n",
    "    hp = dict(hidden=hidden, layers=layers, dropout=dropout, lr=lr,\n",
    "              weight_decay=weight_decay, knn_k=knn_k)\n",
    "    val_m, test_m, sec, ei_np, m = run_one_trial(hp)\n",
    "    row = dict(trial=t, **hp, sec=round(sec,1),\n",
    "               val_f1=val_m['f1'], val_acc=val_m['acc'],\n",
    "               test_f1=test_m['f1'], test_acc=test_m['acc'])\n",
    "    records.append(row)\n",
    "    # 베스트 갱신 기준: val_f1 → ties 시 val_acc\n",
    "    if (best_row is None) or (row['val_f1'] > best_row['val_f1'] + 1e-9) or \\\n",
    "       (abs(row['val_f1']-best_row['val_f1'])<1e-9 and row['val_acc']>best_row['val_acc']):\n",
    "        best_row = row\n",
    "        best_model = m\n",
    "        best_ei_np = ei_np\n",
    "    print(f\"[{t}/{len(cands)}] valF1={row['val_f1']:.4f} valAcc={row['val_acc']:.3f}  hp={hp}\")\n",
    "\n",
    "# 결과 저장/표시\n",
    "df_trials = pd.DataFrame(records).sort_values([\"val_f1\",\"val_acc\"], ascending=False)\n",
    "tune_csv = OUTDIR / \"tuning_trials_improved.csv\"\n",
    "df_trials.to_csv(tune_csv, index=False)\n",
    "print(\"\\nSaved tuning summary ->\", tune_csv)\n",
    "display(df_trials.head(10))\n",
    "\n",
    "print(\"\\n# Best HP:\", {k: best_row[k] for k in [\"hidden\",\"layers\",\"dropout\",\"lr\",\"weight_decay\",\"knn_k\"]})\n",
    "\n",
    "# ---------- 베스트 HP로 재학습(그래프 고정=best knn_k) & 평가/저장 ----------\n",
    "# 그래프/데이터 고정\n",
    "ei_np = best_ei_np\n",
    "data_best = Data(\n",
    "    x=data.x.detach().clone(),\n",
    "    y=data.y.detach().clone(),\n",
    "    edge_index=torch.tensor(ei_np, dtype=torch.long, device=DEVICE),\n",
    ")\n",
    "data_best.train_mask = data.train_mask\n",
    "data_best.val_mask   = data.val_mask\n",
    "data_best.test_mask  = data.test_mask\n",
    "\n",
    "# 모델/옵티마\n",
    "hp = {k: best_row[k] for k in [\"hidden\",\"layers\",\"dropout\",\"lr\",\"weight_decay\",\"knn_k\"]}\n",
    "model_best = ResGCN(\n",
    "    in_dim=data_best.x.size(1),\n",
    "    hidden=int(hp['hidden']),\n",
    "    out_dim=num_classes,\n",
    "    layers=int(hp['layers']),\n",
    "    dropout=float(hp['dropout'])\n",
    ").to(DEVICE)\n",
    "opt = torch.optim.Adam(model_best.parameters(), lr=float(hp['lr']), weight_decay=float(hp['weight_decay']))\n",
    "\n",
    "# 학습\n",
    "best_val, best_state, stale = -1.0, None, 0\n",
    "for ep in range(1, int(IMPROVED_HP['epochs']) + 1):\n",
    "    model_best.train(); opt.zero_grad()\n",
    "    logits = model_best(data_best)\n",
    "    loss = ce_with_smoothing(logits[data_best.train_mask], data_best.y[data_best.train_mask])\n",
    "    loss.backward(); opt.step()\n",
    "    model_best.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_eval = model_best(data_best)\n",
    "        val = evaluate_subset(logits_eval[data_best.val_mask], data_best.y[data_best.val_mask], num_classes)\n",
    "    if val['f1'] > best_val + 1e-6:\n",
    "        best_val = val['f1']\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model_best.state_dict().items()}\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale >= PATIENCE:\n",
    "            break\n",
    "if best_state is not None:\n",
    "    model_best.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "# 평가\n",
    "model_best.eval()\n",
    "with torch.no_grad():\n",
    "    logits_fin = model_best(data_best)\n",
    "    test_m = evaluate_subset(logits_fin[data_best.test_mask], data_best.y[data_best.test_mask], num_classes)\n",
    "\n",
    "print(\"\\n# Internal Test (macro) — Improved+BestHP\")\n",
    "for k in ['acc','prec','rec','f1','roc_auc','pr_auc']:\n",
    "    print(f\"{k.upper():>8}: {test_m[k]:.4f}\")\n",
    "\n",
    "# 혼동행렬 저장\n",
    "cm = confusion_matrix(\n",
    "    data_best.y[data_best.test_mask].detach().cpu().numpy(),\n",
    "    logits_fin[data_best.test_mask].argmax(-1).detach().cpu().numpy()\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(cm, interpolation='nearest'); ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_title('Confusion Matrix — Internal Test (Improved+BestHP)')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "ax.set_xticks(range(num_classes)); ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([c[:14] for c in list(le.classes_)], rotation=45, ha='right')\n",
    "ax.set_yticklabels([c[:14] for c in list(le.classes_)])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, int(v), ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "cm_path = OUTDIR / 'cm_internal_improved_besthp.png'\n",
    "plt.savefig(cm_path, dpi=220); plt.close(fig)\n",
    "print(\"Saved CM ->\", cm_path)\n",
    "\n",
    "# 모델 저장\n",
    "ckpt_path = OUTDIR / 'resgcn_improved_besthp.pt'\n",
    "torch.save({\n",
    "    'state_dict': model_best.state_dict(),\n",
    "    'hp': hp,\n",
    "    'knn_k': int(hp['knn_k']),\n",
    "    'config': {\n",
    "        'in_dim': int(data_best.x.size(1)),\n",
    "        'out_dim': int(num_classes),\n",
    "        'label_mapping': dict(enumerate([str(c) for c in list(le.classes_)])),\n",
    "        'metric': KNN_METRIC,\n",
    "        'embedder': EMBEDDER_NAME,\n",
    "        'mutual_kNN': True\n",
    "    },\n",
    "    'class_weights': class_weights.detach().cpu().numpy().tolist(),\n",
    "    'label_smoothing': float(LABEL_SMOOTHING)\n",
    "}, ckpt_path)\n",
    "size_mb = ckpt_path.stat().st_size / (1024*1024)\n",
    "print(f\"Saved model -> {ckpt_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# 메트릭 요약에 추가\n",
    "row = {\n",
    "    \"Model\": \"Improved+BestHP\",\n",
    "    \"acc\": test_m['acc'],\n",
    "    \"prec\": test_m['prec'],\n",
    "    \"rec\": test_m['rec'],\n",
    "    \"f1\": test_m['f1'],\n",
    "    \"roc_auc\": test_m['roc_auc'],\n",
    "    \"pr_auc\": test_m['pr_auc'],\n",
    "    \"ModelSizeMB\": round(size_mb, 2),\n",
    "    \"kNN_k\": int(hp['knn_k']),\n",
    "    \"embedder\": EMBEDDER_NAME,\n",
    "    \"mutual_kNN\": True,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING\n",
    "}\n",
    "sum_csv = OUTDIR / 'metrics_internal_summary.csv'\n",
    "if sum_csv.exists():\n",
    "    df_old = pd.read_csv(sum_csv)\n",
    "    df_new = pd.concat([df_old, pd.DataFrame([row])], ignore_index=True)\n",
    "else:\n",
    "    df_new = pd.DataFrame([row])\n",
    "df_new.to_csv(sum_csv, index=False)\n",
    "print(\"Updated metrics ->\", sum_csv)\n",
    "display(df_new.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e289796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Using best_model from tuning (no retrain)\n",
      " Best HP: {'hidden': 128, 'layers': 2, 'dropout': 0.1, 'lr': 0.001, 'weight_decay': 0.001, 'knn_k': 15}\n",
      "\n",
      "Validation (should match table ±ε): F1=0.8268, ACC=0.9219\n",
      "Test        (should match table ±ε): F1=0.8415, ACC=0.9220\n",
      "Saved CM -> ../resgcn_runs/cm_internal_improved_besthp_from_tuning.png\n",
      "Saved model -> ../resgcn_runs/resgcn_improved_besthp_from_tuning.pt (0.83 MB)\n",
      "Updated metrics -> ../resgcn_runs/metrics_internal_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>ModelSizeMB</th>\n",
       "      <th>kNN_k</th>\n",
       "      <th>embedder</th>\n",
       "      <th>mutual_kNN</th>\n",
       "      <th>label_smoothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Improved</td>\n",
       "      <td>0.939158</td>\n",
       "      <td>0.882171</td>\n",
       "      <td>0.875349</td>\n",
       "      <td>0.876785</td>\n",
       "      <td>0.990689</td>\n",
       "      <td>0.922132</td>\n",
       "      <td>0.83</td>\n",
       "      <td>10</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Improved+BestHP</td>\n",
       "      <td>0.648986</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.244827</td>\n",
       "      <td>0.241957</td>\n",
       "      <td>0.960709</td>\n",
       "      <td>0.756355</td>\n",
       "      <td>0.83</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improved+BestHP(tuning)</td>\n",
       "      <td>0.921997</td>\n",
       "      <td>0.839287</td>\n",
       "      <td>0.848213</td>\n",
       "      <td>0.841466</td>\n",
       "      <td>0.987748</td>\n",
       "      <td>0.900755</td>\n",
       "      <td>0.83</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model       acc      prec       rec        f1   roc_auc  \\\n",
       "0                 Improved  0.939158  0.882171  0.875349  0.876785  0.990689   \n",
       "1          Improved+BestHP  0.648986  0.310345  0.244827  0.241957  0.960709   \n",
       "2  Improved+BestHP(tuning)  0.921997  0.839287  0.848213  0.841466  0.987748   \n",
       "\n",
       "     pr_auc  ModelSizeMB  kNN_k                                 embedder  \\\n",
       "0  0.922132         0.83     10  sentence-transformers/all-mpnet-base-v2   \n",
       "1  0.756355         0.83     15  sentence-transformers/all-mpnet-base-v2   \n",
       "2  0.900755         0.83     15  sentence-transformers/all-mpnet-base-v2   \n",
       "\n",
       "   mutual_kNN  label_smoothing  \n",
       "0        True             0.05  \n",
       "1        True             0.05  \n",
       "2        True             0.05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 11 — Hotfix: use best_model from tuning (no retrain), evaluate & save\n",
    "\n",
    "import numpy as np, pandas as pd, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, roc_auc_score, average_precision_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# 0) 가드: 이전 셀의 결과물이 있어야 함\n",
    "assert 'best_model' in globals() and 'best_row' in globals() and 'best_ei_np' in globals(), \\\n",
    "    \"tuning 셀을 먼저 실행해 best_model/best_row/best_ei_np가 메모리에 있어야 합니다.\"\n",
    "\n",
    "# 1) best knn_k로 데이터 고정\n",
    "ei_np = best_ei_np\n",
    "data_best = Data(\n",
    "    x=data.x.detach().clone(),\n",
    "    y=data.y.detach().clone(),\n",
    "    edge_index=torch.tensor(ei_np, dtype=torch.long, device=DEVICE),\n",
    ")\n",
    "data_best.train_mask = data.train_mask\n",
    "data_best.val_mask   = data.val_mask\n",
    "data_best.test_mask  = data.test_mask\n",
    "\n",
    "# 2) best_model 바로 평가\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_fin = best_model(data_best)\n",
    "\n",
    "def evaluate_subset(logits, y_true, n_classes):\n",
    "    y_np = y_true.detach().cpu().numpy()\n",
    "    y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "    acc  = accuracy_score(y_np, y_pred)\n",
    "    prec = precision_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    rec  = recall_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    f1   = f1_score(y_np, y_pred, average='macro', zero_division=0)\n",
    "    try:\n",
    "        y_ovr = label_binarize(y_np, classes=np.arange(n_classes))\n",
    "        y_prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        roc = roc_auc_score(y_ovr, y_prob, average='macro', multi_class='ovr')\n",
    "        pr  = average_precision_score(y_ovr, y_prob, average='macro')\n",
    "    except Exception:\n",
    "        roc, pr = float('nan'), float('nan')\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc_auc=roc, pr_auc=pr)\n",
    "\n",
    "val_m  = evaluate_subset(logits_fin[data_best.val_mask],  data_best.y[data_best.val_mask],  num_classes)\n",
    "test_m = evaluate_subset(logits_fin[data_best.test_mask], data_best.y[data_best.test_mask], num_classes)\n",
    "\n",
    "print(\"# Using best_model from tuning (no retrain)\")\n",
    "print(\" Best HP:\", {k: best_row[k] for k in ['hidden','layers','dropout','lr','weight_decay','knn_k']})\n",
    "print(\"\\nValidation (should match table ±ε):\",\n",
    "      f\"F1={val_m['f1']:.4f}, ACC={val_m['acc']:.4f}\")\n",
    "print(\"Test        (should match table ±ε):\",\n",
    "      f\"F1={test_m['f1']:.4f}, ACC={test_m['acc']:.4f}\")\n",
    "\n",
    "# 3) 혼동행렬 저장\n",
    "cm = confusion_matrix(\n",
    "    data_best.y[data_best.test_mask].detach().cpu().numpy(),\n",
    "    logits_fin[data_best.test_mask].argmax(-1).detach().cpu().numpy()\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(cm, interpolation='nearest'); ax.figure.colorbar(im, ax=ax)\n",
    "ax.set_title('Confusion Matrix — Internal Test (Improved+BestHP from tuning)')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "ax.set_xticks(range(num_classes)); ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([c[:14] for c in list(le.classes_)], rotation=45, ha='right')\n",
    "ax.set_yticklabels([c[:14] for c in list(le.classes_)])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, int(v), ha='center', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "cm_path = OUTDIR / 'cm_internal_improved_besthp_from_tuning.png'\n",
    "plt.savefig(cm_path, dpi=220); plt.close(fig)\n",
    "print(\"Saved CM ->\", cm_path)\n",
    "\n",
    "# 4) 모델 저장\n",
    "ckpt_path = OUTDIR / 'resgcn_improved_besthp_from_tuning.pt'\n",
    "torch.save({\n",
    "    'state_dict': best_model.state_dict(),\n",
    "    'hp': {k: best_row[k] for k in ['hidden','layers','dropout','lr','weight_decay','knn_k']},\n",
    "    'knn_k': int(best_row['knn_k']),\n",
    "    'config': {\n",
    "        'in_dim': int(data_best.x.size(1)),\n",
    "        'out_dim': int(num_classes),\n",
    "        'label_mapping': dict(enumerate([str(c) for c in list(le.classes_)])),\n",
    "        'metric': KNN_METRIC,\n",
    "        'embedder': EMBEDDER_NAME,\n",
    "        'mutual_kNN': True\n",
    "    },\n",
    "    'class_weights': class_weights.detach().cpu().numpy().tolist(),\n",
    "    'label_smoothing': float(LABEL_SMOOTHING)\n",
    "}, ckpt_path)\n",
    "size_mb = ckpt_path.stat().st_size / (1024*1024)\n",
    "print(f\"Saved model -> {ckpt_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# 5) 메트릭 요약 업데이트\n",
    "row = {\n",
    "    \"Model\": \"Improved+BestHP(tuning)\",\n",
    "    \"acc\": test_m['acc'],\n",
    "    \"prec\": test_m['prec'],\n",
    "    \"rec\": test_m['rec'],\n",
    "    \"f1\": test_m['f1'],\n",
    "    \"roc_auc\": test_m['roc_auc'],\n",
    "    \"pr_auc\": test_m['pr_auc'],\n",
    "    \"ModelSizeMB\": round(size_mb, 2),\n",
    "    \"kNN_k\": int(best_row['knn_k']),\n",
    "    \"embedder\": EMBEDDER_NAME,\n",
    "    \"mutual_kNN\": True,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING\n",
    "}\n",
    "csv_path = OUTDIR / 'metrics_internal_summary.csv'\n",
    "if csv_path.exists():\n",
    "    df_old = pd.read_csv(csv_path)\n",
    "    df_new = pd.concat([df_old, pd.DataFrame([row])], ignore_index=True)\n",
    "else:\n",
    "    df_new = pd.DataFrame([row])\n",
    "df_new.to_csv(csv_path, index=False)\n",
    "print(\"Updated metrics ->\", csv_path)\n",
    "display(df_new.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "542d0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved report -> ../resgcn_runs/classification_report_internal_besthp.txt\n",
      "Saved ROC -> ../resgcn_runs/roc_curves_internal_besthp.png\n",
      "Saved PR -> ../resgcn_runs/pr_curves_internal_besthp.png\n",
      "Saved bars -> ../resgcn_runs/per_class_bars_internal_besthp.png\n",
      "Saved graph(full) -> ../resgcn_runs/graph_internal_full_besthp.png\n",
      "Saved graph(overlay) -> ../resgcn_runs/graph_internal_test_overlay_besthp.png\n",
      "\n",
      "Done. Files written under: ../resgcn_runs\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## BestHP (from tuning) — visuals & curves & report (internal)\n",
    "\n",
    "import numpy as np, pandas as pd, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt, networkx as nx\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_curve, auc, precision_recall_curve,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, average_precision_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# ---------- 0) 준비: 모델/데이터 확보 ----------\n",
    "# OUTDIR / label encoder / classes\n",
    "classes = list(le.classes_)\n",
    "n_classes = len(classes)\n",
    "\n",
    "# best knn_k의 edge_index / data_best 확보\n",
    "if 'best_ei_np' not in globals():\n",
    "    # 튜닝을 안 거친 세션이라면 HP와 모델을 파일에서 복구\n",
    "    ckpt = OUTDIR / 'resgcn_improved_besthp_from_tuning.pt'\n",
    "    assert ckpt.exists(), \"best_ei_np가 없고 ckpt도 없습니다. 튜닝/핫픽스 셀을 먼저 실행하세요.\"\n",
    "    ck = torch.load(ckpt, map_location='cpu')\n",
    "    knn_k = int(ck.get('knn_k', 15))\n",
    "    # mutual kNN으로 edge 재구성\n",
    "    idxs = build_knn_indices(X, k=knn_k, metric=KNN_METRIC)\n",
    "    best_ei_np = build_edge_index_from_neighbors(idxs, mutual=True)\n",
    "\n",
    "data_best = Data(\n",
    "    x=data.x.detach().clone(),\n",
    "    y=data.y.detach().clone(),\n",
    "    edge_index=torch.tensor(best_ei_np, dtype=torch.long, device=DEVICE),\n",
    ")\n",
    "data_best.train_mask = data.train_mask\n",
    "data_best.val_mask   = data.val_mask\n",
    "data_best.test_mask  = data.test_mask\n",
    "\n",
    "# best_model 확보 (메모리 없으면 ckpt에서 로드)\n",
    "if 'best_model' not in globals():\n",
    "    from copy import deepcopy\n",
    "    ckpt = OUTDIR / 'resgcn_improved_besthp_from_tuning.pt'\n",
    "    ck = torch.load(ckpt, map_location=DEVICE)\n",
    "    hp = ck['hp']\n",
    "    model_tmp = ResGCN(\n",
    "        in_dim=int(data_best.x.size(1)),\n",
    "        hidden=int(hp['hidden']),\n",
    "        out_dim=n_classes,\n",
    "        layers=int(hp['layers']),\n",
    "        dropout=float(hp['dropout'])\n",
    "    ).to(DEVICE)\n",
    "    model_tmp.load_state_dict(ck['state_dict'])\n",
    "    best_model = model_tmp\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_all = best_model(data_best)\n",
    "\n",
    "test_mask = data_best.test_mask.detach().cpu().numpy().astype(bool)\n",
    "y_true = data_best.y.detach().cpu().numpy()[test_mask]\n",
    "probs  = F.softmax(logits_all, dim=-1).detach().cpu().numpy()[test_mask]\n",
    "y_pred = probs.argmax(1)\n",
    "\n",
    "# ---------- 1) 리포트/지표 저장 ----------\n",
    "report_txt = classification_report(y_true, y_pred, target_names=classes, zero_division=0)\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "y_ovr = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "roc  = roc_auc_score(y_ovr, probs, average='macro', multi_class='ovr')\n",
    "pr   = average_precision_score(y_ovr, probs, average='macro')\n",
    "\n",
    "rep_path = OUTDIR / 'classification_report_internal_besthp.txt'\n",
    "with open(rep_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_txt + \"\\n\\n\")\n",
    "    f.write(f\"ACC={acc:.4f}  PREC={prec:.4f}  REC={rec:.4f}  F1={f1:.4f}\\n\")\n",
    "    f.write(f\"ROC_AUC={roc:.4f}  PR_AUC={pr:.4f}\\n\")\n",
    "print(\"Saved report ->\", rep_path)\n",
    "\n",
    "# ---------- 2) ROC / PR 커브 ----------\n",
    "# per-class curves + micro/macro\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "prec_c, rec_c, pr_auc = {}, {}, {}\n",
    "\n",
    "# micro\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_ovr.ravel(), probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "prec_c[\"micro\"], rec_c[\"micro\"], _ = precision_recall_curve(y_ovr.ravel(), probs.ravel())\n",
    "pr_auc[\"micro\"] = auc(rec_c[\"micro\"], prec_c[\"micro\"])\n",
    "\n",
    "# per-class\n",
    "for i, cname in enumerate(classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_ovr[:, i], probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    prec_c[i], rec_c[i], _ = precision_recall_curve(y_ovr[:, i], probs[:, i])\n",
    "    pr_auc[i] = auc(rec_c[i], prec_c[i])\n",
    "\n",
    "# macro = per-class 평균\n",
    "roc_auc[\"macro\"] = np.mean([roc_auc[i] for i in range(n_classes)])\n",
    "pr_auc[\"macro\"]  = np.mean([pr_auc[i]  for i in range(n_classes)])\n",
    "\n",
    "# Plot ROC\n",
    "fig = plt.figure(figsize=(10,7)); ax = fig.add_subplot(111)\n",
    "ax.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2.5, label=f\"micro-average (AUC={roc_auc['micro']:.3f})\")\n",
    "for i, cname in enumerate(classes):\n",
    "    ax.plot(fpr[i], tpr[i], lw=1.2, alpha=0.8, label=f\"{cname[:18]} (AUC={roc_auc[i]:.3f})\")\n",
    "ax.plot([0,1],[0,1],'k--',lw=1)\n",
    "ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — Internal Test (BestHP)')\n",
    "ax.legend(bbox_to_anchor=(1.02,0.5), loc='center left', frameon=False)\n",
    "plt.tight_layout()\n",
    "roc_path = OUTDIR / 'roc_curves_internal_besthp.png'\n",
    "plt.savefig(roc_path, dpi=220, bbox_inches='tight'); plt.close(fig)\n",
    "print(\"Saved ROC ->\", roc_path)\n",
    "\n",
    "# Plot PR\n",
    "fig = plt.figure(figsize=(10,7)); ax = fig.add_subplot(111)\n",
    "ax.plot(rec_c[\"micro\"], prec_c[\"micro\"], lw=2.5, label=f\"micro-average (AUC={pr_auc['micro']:.3f})\")\n",
    "for i, cname in enumerate(classes):\n",
    "    ax.plot(rec_c[i], prec_c[i], lw=1.2, alpha=0.8, label=f\"{cname[:18]} (AUC={pr_auc[i]:.3f})\")\n",
    "ax.set_xlabel('Recall'); ax.set_ylabel('Precision')\n",
    "ax.set_title('PR Curves — Internal Test (BestHP)')\n",
    "ax.legend(bbox_to_anchor=(1.02,0.5), loc='center left', frameon=False)\n",
    "plt.tight_layout()\n",
    "pr_path = OUTDIR / 'pr_curves_internal_besthp.png'\n",
    "plt.savefig(pr_path, dpi=220, bbox_inches='tight'); plt.close(fig)\n",
    "print(\"Saved PR ->\", pr_path)\n",
    "\n",
    "# ---------- 3) Per-class bar (Precision/Recall/F1) ----------\n",
    "per_prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "per_rec  = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "per_f1   = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x = np.arange(n_classes)\n",
    "ax.bar(x-0.25, per_prec, width=0.25, label='Precision')\n",
    "ax.bar(x,       per_rec,  width=0.25, label='Recall')\n",
    "ax.bar(x+0.25,  per_f1,   width=0.25, label='F1')\n",
    "ax.set_xticks(x); ax.set_xticklabels([c[:14] for c in classes], rotation=45, ha='right')\n",
    "ax.set_ylim(0,1.05); ax.legend()\n",
    "ax.set_title('Per-class metrics — Internal Test (BestHP)')\n",
    "plt.tight_layout()\n",
    "bar_path = OUTDIR / 'per_class_bars_internal_besthp.png'\n",
    "plt.savefig(bar_path, dpi=220); plt.close(fig)\n",
    "print(\"Saved bars ->\", bar_path)\n",
    "\n",
    "# ---------- 4) 그래프 시각화 (BestHP: k=best, mutual) ----------\n",
    "# 레이아웃은 best 그래프 기반으로 새로 계산\n",
    "G = nx.Graph(); G.add_nodes_from(range(X.shape[0])); G.add_edges_from(list(zip(best_ei_np[0], best_ei_np[1])))\n",
    "pos = nx.spring_layout(G, seed=RNG_SEED, iterations=50)\n",
    "coords_best = np.array([pos[i] for i in range(X.shape[0])])\n",
    "\n",
    "# 전체\n",
    "fig = plt.figure(figsize=(16,10)); ax = fig.add_subplot(111)\n",
    "ax.set_title(f\"kNN Graph (mutual=True, k={int(best_row['knn_k'])}) — all nodes (BestHP)\")\n",
    "# edges\n",
    "for (u,v) in zip(best_ei_np[0], best_ei_np[1]):\n",
    "    ax.plot([coords_best[u,0], coords_best[v,0]],[coords_best[u,1], coords_best[v,1]], alpha=0.03)\n",
    "# nodes by class\n",
    "for ci, cname in enumerate(classes):\n",
    "    idx = np.where(data_best.y.detach().cpu().numpy() == ci)[0]\n",
    "    if len(idx): ax.scatter(coords_best[idx,0], coords_best[idx,1], s=8, alpha=0.85, label=cname)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0,0.5), frameon=False)\n",
    "ax.axis('off'); plt.tight_layout()\n",
    "full_best_path = OUTDIR / 'graph_internal_full_besthp.png'\n",
    "plt.savefig(full_best_path, dpi=220, bbox_inches='tight'); plt.close(fig)\n",
    "print(\"Saved graph(full) ->\", full_best_path)\n",
    "\n",
    "# 테스트 오버레이 (정답/오답 테두리)\n",
    "fig = plt.figure(figsize=(16,10)); ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Internal Test Overlay — BestHP\")\n",
    "for (u,v) in zip(best_ei_np[0], best_ei_np[1]):\n",
    "    ax.plot([coords_best[u,0], coords_best[v,0]],[coords_best[u,1], coords_best[v,1]], alpha=0.02, color='gray')\n",
    "ax.scatter(coords_best[:,0], coords_best[:,1], s=6, alpha=0.1, c='lightgray')\n",
    "tm = test_mask\n",
    "# per class test nodes (큰 점 + 검정 테두리, 정답은 초록/오답은 빨강 테두리)\n",
    "test_pred = y_pred\n",
    "test_true = y_true\n",
    "correct = (test_pred == test_true)\n",
    "for ci, cname in enumerate(classes):\n",
    "    idx = np.where((data_best.y.detach().cpu().numpy()==ci) & tm)[0]\n",
    "    if len(idx):\n",
    "        edgecols = ['k']*len(idx)\n",
    "        ax.scatter(coords_best[idx,0], coords_best[idx,1], s=140, edgecolor='k', linewidths=0.9, label=cname)\n",
    "# 정답/오답 테두리 강조\n",
    "idx_test = np.where(tm)[0]\n",
    "ax.scatter(coords_best[idx_test[ correct],0], coords_best[idx_test[ correct],1],\n",
    "           s=160, facecolors='none', edgecolors='lime', linewidths=1.8, label='Correct')\n",
    "ax.scatter(coords_best[idx_test[~correct],0], coords_best[idx_test[~correct],1],\n",
    "           s=160, facecolors='none', edgecolors='red',  linewidths=1.8, label='Misclassified')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1.0,0.5), frameon=False)\n",
    "ax.axis('off'); plt.tight_layout()\n",
    "overlay_best_path = OUTDIR / 'graph_internal_test_overlay_besthp.png'\n",
    "plt.savefig(overlay_best_path, dpi=220, bbox_inches='tight'); plt.close(fig)\n",
    "print(\"Saved graph(overlay) ->\", overlay_best_path)\n",
    "\n",
    "print(\"\\nDone. Files written under:\", OUTDIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
